{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divassya/BigDataAnalysis/blob/main/AssiyaKaratay_Assignment_4_Wiki_Categories_SparkDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWtfJiBQVj0"
      },
      "source": [
        "#### Info \n",
        "Assignment 4\n",
        "MET CS777 Big Data Analytics\n",
        "\n",
        "Faculty - Farshid Alizadeh-Shabdiz, PhD, MBA\n",
        "\n",
        "Student - Assiya Karatay U95161396 karatay@bu.edu 857-294-7028"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_-f0u7hQpVh"
      },
      "source": [
        "#### import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_0d-ecHfQTO6"
      },
      "outputs": [],
      "source": [
        "!pip install --ignore-installed -q pyspark==3.1.2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k79iPwESQWG6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import add\n",
        "import re\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from pyspark import SparkContext, SQLContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.functions import udf, col,monotonically_increasing_id\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhSls3S9RFsH"
      },
      "source": [
        "#### set up the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpM2mlTxRBFG",
        "outputId": "abbda045-3fda-401b-ce58-a9d4050906af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#### set up the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYaoBzWuRKtd",
        "outputId": "e71c8891-c383-4d64-89a4-802997f58b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Working directory was changed to /content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\n"
          ]
        }
      ],
      "source": [
        "# choose where project files will be saved\n",
        "project_folder = \"/content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\"\n",
        "# project_folder = sys.argv[2]\n",
        "# change the OS to use the project folder as the working directory\n",
        "os.chdir(project_folder)\n",
        "\n",
        "print('\\n Working directory was changed to ' + project_folder )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jGqYYTpURcIr"
      },
      "outputs": [],
      "source": [
        "# 1. get the file\n",
        "wikiPagesFile= project_folder + \"WikipediaPagesOneDocPerLine1000LinesSmall.txt.bz2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M_0jxcF9UlIQ"
      },
      "outputs": [],
      "source": [
        "wikiPages = spark.read.format('csv')\\\n",
        ".options(header='false', inferSchema='true', sep='|')\\\n",
        ".load(wikiPagesFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qaJ2uimvUWOS"
      },
      "outputs": [],
      "source": [
        "# Each entry in validLines will be a line from the text file\n",
        "validLines = wikiPages.filter(wikiPages['_c0'].contains('id' and 'url='))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRW5zOGor0yC"
      },
      "source": [
        "### Task 1 Generate a 20K dictionary (10 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd6fR5hxr4yz"
      },
      "source": [
        "#### 1.1 The top 20,000 English words\n",
        "Using Wikipedia pages, find the top 20,000 English words, save them in an array, and sort them based on the frequency of the occurrence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Au63nF0ZZPc5"
      },
      "outputs": [],
      "source": [
        "def getID(lines):\n",
        "  # divide the data defore and after url\n",
        "  division = lines.split('\" url')\n",
        "  # strip all characters except DOC ID\n",
        "  id = division[0].split('<doc id=\"')[1] \n",
        "  return id\n",
        "\n",
        "\n",
        "def getText(lines):\n",
        "  # divide the data defore and after url\n",
        "  division = lines.split('\" url')\n",
        "  # strip the end of title and last 6 characters containing '.</doc' .\n",
        "  text = division[1].split('\">')[1][:-6]\n",
        "  # check the UNICODE decoding for readability \n",
        "  # and lowercase to count words with the same letters together\n",
        "  regex = re.compile(r'[^a-zA-Z]', re.UNICODE).split(text.lower())\n",
        "  return regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H9Ev7ouSaYHX"
      },
      "outputs": [],
      "source": [
        "# Converting function to UDF\n",
        "getIDUDF = udf(getID, StringType())\n",
        "getTextUDF = udf(getText, ArrayType(StringType()))\n",
        "# derive two columns of out the one text column\n",
        "docID = validLines.withColumn('docID', getIDUDF('_c0'))\n",
        "docIDAndListOfWords = docID.withColumn('text', getTextUDF('_c0')).select('docID', 'text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kGUNqrtqaYn4"
      },
      "outputs": [],
      "source": [
        "# drops docID \n",
        "# explodes text to (word,1) pairs to count the number of occurrence\n",
        "# get top 20K most common words in the corpus\n",
        "\n",
        "topWords = docIDAndListOfWords.withColumn('word', f.explode(f.col('text')))\\\n",
        "    .groupBy('word')\\\n",
        "    .count()\\\n",
        "    .filter('word != \"\"')\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .limit(20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KZcZdbBhAzLt"
      },
      "outputs": [],
      "source": [
        "# add a new column of dict pos\n",
        "dictionary = topWords.withColumn('dictNum', monotonically_increasing_id()).drop('count')\n",
        "# dictionary.write.format(\"csv\")\\\n",
        "# .mode(\"overwrite\")\\\n",
        "# .option(\"header\",True)\\\n",
        "# .save(project_folder + \"df_results/task11_dict/\")\n",
        "# print(dictionary.show(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZPA_UldtLE5"
      },
      "source": [
        "#### 1.2 docID as key and a Numpy array for the position of each word\n",
        "As a result, a dictionary has been generated that contains the top 20K most frequent words in the corpus. Next go over each Wikipedia document and check if the words appear in the Top 20K words. At the end, produce an RDD that includes\n",
        "the docID as key and a Numpy array for the position of each word in the top 20K dictionary.\n",
        "(docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a5XeUxCFtKsN"
      },
      "outputs": [],
      "source": [
        "# Get a DF that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
        "# (\"word1\", docID), (\"word2\", docId), ...\n",
        "allWordsWithDocID = docIDAndListOfWords.withColumn('word', f.explode('text'))\\\n",
        ".drop('text').filter('word != \"\"')\n",
        "# join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
        "allDictionaryWords = dictionary.join(allWordsWithDocID, 'word')\n",
        "# Drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
        "docIDAndPos = allDictionaryWords.drop('word')\n",
        "# Get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
        "# Group by key and use collect_set to combine dictNum for each key\n",
        "allDictionaryWordsInEachDoc = docIDAndPos.groupBy('docID')\\\n",
        ".agg(f.collect_set('dictNum').alias('dictWordsList'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mkjsbZlMQTk7"
      },
      "outputs": [],
      "source": [
        "# allDictionaryWordsInEachDoc.write\\\n",
        "# .mode(\"overwrite\")\\\n",
        "# .option(\"header\",True)\\\n",
        "# .parquet(project_folder + \"df_results/task12_wordOccurrences/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgOMqFCLNbO5"
      },
      "source": [
        "### Task 2 - Create the TF-IDF Array (20 Points)\n",
        "#### TF\n",
        "After having the top 20K words we want to create a large array that its\n",
        "columns are the words of the dictionary with number of occurrences of each word and the rows are documents.\n",
        "The first step is calculating the “Term Frequency”, TF (x, w), vector for each document as follows:\n",
        "“Term Frequency” is an indication of the number of times a term occurs in a document.\n",
        "Numerator is number of occurrences of a word, and the denominator is the sum of all the words of the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4iuiKOHXwTmQ"
      },
      "outputs": [],
      "source": [
        "def tf(listOfIndices):\n",
        "    # create an array of zeros\n",
        "    returnVal = np.zeros(20000)\n",
        "    # count the occurrence of words e.g. there are 514 'my' in docID1, where \n",
        "    # 'my' is in the position 0 in the corpus\n",
        "    for index in listOfIndices:\n",
        "        returnVal[index] = returnVal[index] + 1\n",
        "    numberOfWords = np.sum(listOfIndices)\n",
        "    returnVal = np.divide(returnVal, numberOfWords)\n",
        "    return returnVal.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WeYeFyAdGTL5"
      },
      "outputs": [],
      "source": [
        "tfUDF = udf(tf, ArrayType(FloatType(), containsNull=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7rQBk-OBHFb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb6d5a0-dc85-424b-8600-51a4d94a686e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "| docID|                  tf|\n",
            "+------+--------------------+\n",
            "|434061|[3.0526055E-6, 3....|\n",
            "|455037|[0.0, 0.0, 0.0, 0...|\n",
            "+------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The following line this gets us a set of\n",
        "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
        "# and converts the dictionary positions to a bag-of-words numpy array...\n",
        "# use the buildArray function to build the feature array\n",
        "# regexp_replace is used to remove square brackets from buildArray output\n",
        "tfArrays = allDictionaryWordsInEachDoc\\\n",
        ".withColumn('tf',tfUDF('dictWordsList'))\\\n",
        ".drop('dictWordsList')\n",
        "\n",
        "tfArrays.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naubHBEHRZb8"
      },
      "source": [
        "#### IDF\n",
        "Next, calculate “Inverse Document Frequency” for all the documents and finally\n",
        "calculate TF-IDF(w) and create TF-IDF matrix of the corpus:\n",
        "Note that the “size of corpus” is total number of documents (numerator).\n",
        "To learn more about TF-IDF see the Wikipedia page:\n",
        "https://en.wikipedia.org/wiki/Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rtKzjWNyHFhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8bf27e-4ef9-44b5-a894-3636e6c29a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  5 105 526 ...   1   1   1]\n"
          ]
        }
      ],
      "source": [
        "# the i^th entry tells us how many\n",
        "# individual documents the i^th word in the dictionary appeared in\n",
        "dfArray = np.array(docIDAndPos.distinct()\\\n",
        "                   .groupBy('dictNum')\\\n",
        "                   .agg(f.count('docID'))\\\n",
        "                   .drop('dictNum')\n",
        "                   .collect()).flatten()\n",
        "print(dfArray)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mYdsW30rHFkv"
      },
      "outputs": [],
      "source": [
        "# Get the version of dfArray where the i^th entry is the inverse-document frequency for the\n",
        "# i^th word in the corpus\n",
        "numberOfDocs = wikiPages.count()\n",
        "idfArray = np.log(np.divide(np.full(20000, numberOfDocs), dfArray))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Hat-C0_-WcpR"
      },
      "outputs": [],
      "source": [
        "multiply_idfArray = udf(lambda x: np.multiply(x, idfArray).tolist(), ArrayType(FloatType(), containsNull=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glXJGWulR3yk",
        "outputId": "f5249196-02b4-4287-8a1d-5dcd576fca01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "| docID|               TFidf|\n",
            "+------+--------------------+\n",
            "|434061|[1.6173673E-5, 6....|\n",
            "|455037|[0.0, 0.0, 0.0, 0...|\n",
            "+------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Finally, convert all of the tf vectors in allDocsAsNumpyArrays to tf * idf vectors and remove []\n",
        "# Then split by ,\n",
        "allDocsAsNumpyArraysTFidf = tfArrays.withColumn('TFidf', multiply_idfArray('tf'))\\\n",
        ".drop('tf')\n",
        "\n",
        "print(allDocsAsNumpyArraysTFidf.show(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1XMY_fmrXsVT"
      },
      "outputs": [],
      "source": [
        "allDocsAsNumpyArraysTFidf.write\\\n",
        ".mode(\"overwrite\")\\\n",
        ".option(\"header\",True)\\\n",
        ".parquet(project_folder + \"/df_results/task2_tfidf/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2hJPDsOXzep"
      },
      "source": [
        "### Task 3 - Implement the getPrediction function (30 Points)\n",
        "Finally, implement the function getPrediction(textInput, k), which will predict the\n",
        "membership of the textInput to the top 20 closest documents, and the list of top\n",
        "categories.\n",
        "You should use the cosine similarity to calculate the distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flzU9sulR3l0",
        "outputId": "b7186b0f-d46f-45a1-903e-6b224956ffe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|   _c0|                 _c1|\n",
            "+------+--------------------+\n",
            "|434042|   1987_debut_albums|\n",
            "|434042|Albums_produced_b...|\n",
            "+------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "wikiCategoryFile = project_folder + \"wiki-categorylinks-small.csv.bz2\"\n",
        "\n",
        "wikiCats=spark.read.format('csv')\\\n",
        ".options(header = 'false', inferSchema = 'true', sep = ',')\\\n",
        ".load(wikiCategoryFile)\n",
        "wikiCats.show(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6gcN0-lV3Ue",
        "outputId": "ce9f7c63-b6de-429b-c6f9-a5c4ed7a4c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|            category|               TFidf|\n",
            "+--------------------+--------------------+\n",
            "|Use_dmy_dates_fro...|[1.6173673E-5, 6....|\n",
            "|Politics_of_East_...|[1.6173673E-5, 6....|\n",
            "|Lists_of_politica...|[1.6173673E-5, 6....|\n",
            "|Leaders_of_East_G...|[1.6173673E-5, 6....|\n",
            "|East_Germany_poli...|[1.6173673E-5, 6....|\n",
            "|Articles_lacking_...|[1.6173673E-5, 6....|\n",
            "|Articles_containi...|[1.6173673E-5, 6....|\n",
            "|All_articles_lack...|[1.6173673E-5, 6....|\n",
            "|Human_name_disamb...|[0.0, 0.0, 0.0, 0...|\n",
            "|Disambiguation_pa...|[0.0, 0.0, 0.0, 0...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now, we join it with categories, and map it after join so that we have only the wikipageID \n",
        "# This joun can take time on your laptop. \n",
        "# You can do the join once and generate a new wikiCats data and store it. Our WikiCategories includes all categories\n",
        "# of wikipedia. \n",
        "\n",
        "wikiAndCatsJoined = wikiCats.withColumnRenamed('_c0', 'docID')\\\n",
        "            .withColumnRenamed('_c1', 'category')\\\n",
        "            .join(allDocsAsNumpyArraysTFidf, on='docID')\n",
        "featuresDF = wikiAndCatsJoined.select('category', 'TFidf')\n",
        "\n",
        "# Cache this important data because we need to run kNN on this data set. \n",
        "featuresDF.cache()\n",
        "featuresDF.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "U1eMy63MR3pL"
      },
      "outputs": [],
      "source": [
        "def cosineSim (x,y):\n",
        "\tnormA = np.linalg.norm(x)\n",
        "\tnormB = np.linalg.norm(y)\n",
        "\treturn (round(np.dot(x,y)/(normA*normB),3)).tolist()\n",
        "\n",
        "cosinSim_udf = udf(cosineSim, FloatType())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "elU_EIt3VivE"
      },
      "outputs": [],
      "source": [
        "# Assumption: Each document is stored in one line of the text file\n",
        "# We need this count later ... \n",
        "numberOfDocs = wikiPages.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XKN9YQyXYLkS"
      },
      "outputs": [],
      "source": [
        "def getPrediction(textInput,k):\n",
        "    # create a df \n",
        "    df = spark.createDataFrame([textInput], StringType())\n",
        "    print(df.show(2))\n",
        "    #Flat map the text to (word, 1) pair for each word in the doc\n",
        "    textWords =df.withColumn ('word', f.explode(f.split(f.lower(\n",
        "                                                  f.regexp_replace('value', '[^a-zA-Z]', ' ')), ' '))).\\\n",
        "                                                  withColumn('count', f.lit(1))\\\n",
        "                                                  .filter('word != \"\"')\\\n",
        "                                                  .drop('value')\n",
        "    print(textWords.show(2))      \n",
        "    # This will give us a set of (word, (dictionaryPos, 1)) pairs\n",
        "    allDictionaryWordsInThatDoc = dictionary.join (textWords, on='word').\\\n",
        "                                  select('dictNum', 'count').groupBy('count').\\\n",
        "                                      agg(f.collect_set('dictNum'))\n",
        "    print(allDictionaryWordsInThatDoc.show(2))\n",
        "    #Get tf array for the input string\n",
        "    tfArray = allDictionaryWordsInThatDoc.orderBy('count', ascending = False).limit(1)\\\n",
        "                          .withColumn('tfArray', tfUDF('collect_set(dictNum)')).\\\n",
        "                          select('tfArray') \n",
        "    # Multiply by idfArray\n",
        "    myArray = tfArray.withColumn('tfxIdf', multiply_idfArray('tfArray')).select('tfxIdf')\n",
        "\n",
        "    # Get the tf * idf array for the input string\n",
        "    # Get the distance from the input text string to all database documents, \n",
        "    # using cosine similarity (np.dot() )\n",
        "    distances = featuresDF.join(myArray)\n",
        "    distances = distances.withColumn('dist', cosinSim_udf('TFidf', 'tfxIdf'))\\\n",
        "                            .select('category', 'dist')\n",
        "        # print(distances.show(2))\n",
        "    \n",
        "    # get the top k distances\n",
        "    topK = distances.orderBy('dist', ascending = False).limit(k)\n",
        "    # print(topK.show(5))\n",
        "\n",
        "    # now, for each docID, get the count of the number of times this document ID appeared in the top k\n",
        "    numTimes = topK.groupBy('category')\\\n",
        "                  .agg(f.count('category').alias('count'))\\\n",
        "                  .drop('dist')\n",
        "    # print(numTimes.show(5))\n",
        "\n",
        "    return numTimes.orderBy('count', ascending = False).limit(k).collect()                                                 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEV4WDWbYoHz",
        "outputId": "b2a50b1e-ed8f-4f54-a25d-034e5e4ece66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|How many goals Va...|\n",
            "+--------------------+\n",
            "\n",
            "None\n",
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| how|    1|\n",
            "|many|    1|\n",
            "+----+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n",
            "+-----+--------------------+\n",
            "|count|collect_set(dictNum)|\n",
            "+-----+--------------------+\n",
            "|    1|[66, 63, 2626, 64...|\n",
            "+-----+--------------------+\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(getPrediction('How many goals Vancouver score last year?', 10))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNZK8XIhgwOLJfVZ/XNVkRu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}