{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divassya/BigDataAnalysis/blob/main/AssiyaKaratay_Assignment_4_Wiki_Categories_SparkDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWtfJiBQVj0"
      },
      "source": [
        "#### Info \n",
        "Assignment 4\n",
        "MET CS777 Big Data Analytics\n",
        "\n",
        "Faculty - Farshid Alizadeh-Shabdiz, PhD, MBA\n",
        "\n",
        "Student - Assiya Karatay U95161396 karatay@bu.edu 857-294-7028"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_-f0u7hQpVh"
      },
      "source": [
        "#### import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0d-ecHfQTO6",
        "outputId": "ae217432-36d4-4652-812f-59440bc6f186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 59 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 44.7 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --ignore-installed -q pyspark==3.1.2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k79iPwESQWG6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import requests\n",
        "import numpy as np\n",
        "from operator import add\n",
        "import re\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from pyspark import SparkContext, SQLContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.functions import udf, col,monotonically_increasing_id\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhSls3S9RFsH"
      },
      "source": [
        "#### set up the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpM2mlTxRBFG",
        "outputId": "0a677eae-43fa-4d08-8624-2a90822ffc85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#### set up the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYaoBzWuRKtd",
        "outputId": "2ece666c-69f3-420f-f9e1-28bdc24dc5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Working directory was changed to /content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\n"
          ]
        }
      ],
      "source": [
        "# choose where project files will be saved\n",
        "project_folder = \"/content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\"\n",
        "# project_folder = sys.argv[2]\n",
        "# change the OS to use the project folder as the working directory\n",
        "os.chdir(project_folder)\n",
        "\n",
        "print('\\n Working directory was changed to ' + project_folder )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGqYYTpURcIr"
      },
      "outputs": [],
      "source": [
        "# 1. get the file\n",
        "wikiPagesFile= project_folder + \"WikipediaPagesOneDocPerLine1000LinesSmall.txt.bz2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RkEfOHUSZ_S"
      },
      "outputs": [],
      "source": [
        "# Read a file into DF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_0jxcF9UlIQ"
      },
      "outputs": [],
      "source": [
        "wikiPages = spark.read.format('csv')\\\n",
        ".options(header='false', inferSchema='true', sep='|')\\\n",
        ".load(wikiPagesFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaJ2uimvUWOS"
      },
      "outputs": [],
      "source": [
        "# Each entry in validLines will be a line from the text file\n",
        "validLines = wikiPages.filter(wikiPages['_c0'].contains('id' and 'url='))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRW5zOGor0yC"
      },
      "source": [
        "### Task 1 Generate a 20K dictionary (10 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd6fR5hxr4yz"
      },
      "source": [
        "#### 1.1 The top 20,000 English words\n",
        "Using Wikipedia pages, find the top 20,000 English words, save them in an array, and sort them based on the frequency of the occurrence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au63nF0ZZPc5"
      },
      "outputs": [],
      "source": [
        "def getID(lines):\n",
        "  # divide the data defore and after url\n",
        "  division = lines.split('\" url')\n",
        "  # strip all characters except DOC ID\n",
        "  id = division[0].split('<doc id=\"')[1] \n",
        "  return id\n",
        "\n",
        "\n",
        "def getText(lines):\n",
        "  # divide the data defore and after url\n",
        "  division = lines.split('\" url')\n",
        "  # strip the end of title and last 6 characters containing '.</doc' .\n",
        "  text = division[1].split('\">')[1][:-6]\n",
        "  # check the UNICODE decoding for readability \n",
        "  # and lowercase to count words with the same letters together\n",
        "  regex = re.compile(r'[^a-zA-Z]', re.UNICODE).split(text.lower())\n",
        "  return regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Ev7ouSaYHX",
        "outputId": "c34ee33e-d898-4c19-b09f-9a09d73c7923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+\n",
            "| docID|                text|\n",
            "+------+--------------------+\n",
            "|431949|[black, people, a...|\n",
            "+------+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Converting function to UDF\n",
        "getIDUDF = udf(getID, StringType())\n",
        "getTextUDF = udf(getText, ArrayType(StringType()))\n",
        "# derive two columns of out the one text column\n",
        "docID = validLines.withColumn('docID', getIDUDF('_c0'))\n",
        "docIDAndListOfWords = docID.withColumn('text', getTextUDF('_c0')).select('docID', 'text')\n",
        "docIDAndListOfWords.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGUNqrtqaYn4",
        "outputId": "5d5cfd4b-4f30-4975-8fd7-07a4e1829398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the|73457|\n",
            "|  of|34004|\n",
            "+----+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# drops docID \n",
        "# explodes text to (word,1) pairs to count the number of occurrence\n",
        "# get top 20K most common words in the corpus\n",
        "\n",
        "topWords = docIDAndListOfWords.withColumn('word', f.explode(f.col('text')))\\\n",
        "    .groupBy('word')\\\n",
        "    .count()\\\n",
        "    .filter('word != \"\"')\\\n",
        "    .sort('count', ascending=False)\\\n",
        "    .limit(20000)\n",
        "topWords.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZcZdbBhAzLt",
        "outputId": "3e4df442-0f7b-4320-9497-1ae9d3b09827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-------+\n",
            "|word|dictNum|\n",
            "+----+-------+\n",
            "| the|      0|\n",
            "|  of|      1|\n",
            "+----+-------+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# add a new column of dict pos\n",
        "dictionary = topWords.withColumn('dictNum', monotonically_increasing_id()).drop('count')\n",
        "dictionary.write.format(\"csv\")\\\n",
        ".mode(\"overwrite\")\\\n",
        ".option(\"header\",True)\\\n",
        ".save(project_folder + \"/df_results/task11_dict/\")\n",
        "print(dictionary.show(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZPA_UldtLE5"
      },
      "source": [
        "#### 1.2 docID as key and a Numpy array for the position of each word\n",
        "As a result, a dictionary has been generated that contains the top 20K most frequent words in the corpus. Next go over each Wikipedia document and check if the words appear in the Top 20K words. At the end, produce an RDD that includes\n",
        "the docID as key and a Numpy array for the position of each word in the top 20K dictionary.\n",
        "(docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5XeUxCFtKsN",
        "outputId": "e942a35a-e198-4c5c-a205-675f65033af9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+\n",
            "| docID|  word|\n",
            "+------+------+\n",
            "|431949| black|\n",
            "|431949|people|\n",
            "+------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Next, we get a DF that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
        "# (\"word1\", docID), (\"word2\", docId), ...\n",
        "allWordsWithDocID = docIDAndListOfWords.withColumn('word', f.explode('text'))\\\n",
        ".drop('text').\\\n",
        "filter('word != \"\"')\n",
        "allWordsWithDocID.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7FxdJSXtKu2",
        "outputId": "9cf22203-2155-4908-89dd-fcf831ef628c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+------+\n",
            "|  word|dictNum| docID|\n",
            "+------+-------+------+\n",
            "| black|    189|431949|\n",
            "|people|    102|431949|\n",
            "+------+-------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
        "allDictionaryWords = dictionary.join(allWordsWithDocID, 'word')\n",
        "allDictionaryWords.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc49zv5CtKxv",
        "outputId": "71df3ace-c0d2-4f73-b970-dcfee83fd274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------+\n",
            "|dictNum| docID|\n",
            "+-------+------+\n",
            "|    189|431949|\n",
            "|    102|431949|\n",
            "+-------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now, we drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
        "docIDAndPos = allDictionaryWords.drop('word')\n",
        "docIDAndPos.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5sVuBp2tK1j",
        "outputId": "62150d95-d3a0-4794-bda6-f9b4b677fffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+\n",
            "| docID|       dictWordsList|\n",
            "+------+--------------------+\n",
            "|434061|[0, 356, 102, 764...|\n",
            "|455037|   [526, 939, 4, 55]|\n",
            "+------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
        "# Group by key and use collect_set to combine dictNum for each key\n",
        "allDictionaryWordsInEachDoc = docIDAndPos.groupBy('docID')\\\n",
        ".agg(f.collect_set('dictNum').alias('dictWordsList'))\n",
        "\n",
        "allDictionaryWordsInEachDoc.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkjsbZlMQTk7"
      },
      "outputs": [],
      "source": [
        "allDictionaryWordsInEachDoc.write\\\n",
        ".mode(\"overwrite\")\\\n",
        ".option(\"header\",True)\\\n",
        ".parquet(project_folder + \"/df_results/task12_wordOccurrences/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgOMqFCLNbO5"
      },
      "source": [
        "### Task 2 - Create the TF-IDF Array (20 Points)\n",
        "#### TF\n",
        "After having the top 20K words we want to create a large array that its\n",
        "columns are the words of the dictionary with number of occurrences of each word and the rows are documents.\n",
        "The first step is calculating the “Term Frequency”, TF (x, w), vector for each document as follows:\n",
        "“Term Frequency” is an indication of the number of times a term occurs in a document.\n",
        "Numerator is number of occurrences of a word, and the denominator is the sum of all the words of the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iuiKOHXwTmQ"
      },
      "outputs": [],
      "source": [
        "def tf(listOfIndices):\n",
        "    # create an array of zeros\n",
        "    returnVal = np.zeros(20000)\n",
        "    # count the occurrence of words e.g. there are 514 'my' in docID1, where \n",
        "    # 'my' is in the position 0 in the corpus\n",
        "    for index in listOfIndices:\n",
        "        returnVal[index] = returnVal[index] + 1\n",
        "    numberOfWords = np.sum(listOfIndices)\n",
        "    returnVal = np.divide(returnVal, numberOfWords)\n",
        "    return returnVal.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeYeFyAdGTL5"
      },
      "outputs": [],
      "source": [
        "tfUDF = udf(tf, ArrayType(FloatType(), containsNull=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rQBk-OBHFb_",
        "outputId": "3f93cc87-6073-448e-f5c3-8d5b3867ab2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+\n",
            "| docID|                  tf|\n",
            "+------+--------------------+\n",
            "|434061|[3.0526055E-6, 3....|\n",
            "|455037|[0.0, 0.0, 0.0, 0...|\n",
            "+------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The following line this gets us a set of\n",
        "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
        "# and converts the dictionary positions to a bag-of-words numpy array...\n",
        "# use the buildArray function to build the feature array\n",
        "# regexp_replace is used to remove square brackets from buildArray output\n",
        "tfArrays = allDictionaryWordsInEachDoc\\\n",
        ".withColumn('tf',tfUDF('dictWordsList'))\\\n",
        ".drop('dictWordsList')\n",
        "\n",
        "tfArrays.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naubHBEHRZb8"
      },
      "source": [
        "#### IDF\n",
        "Next, calculate “Inverse Document Frequency” for all the documents and finally\n",
        "calculate TF-IDF(w) and create TF-IDF matrix of the corpus:\n",
        "Note that the “size of corpus” is total number of documents (numerator).\n",
        "To learn more about TF-IDF see the Wikipedia page:\n",
        "https://en.wikipedia.org/wiki/Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtKzjWNyHFhN",
        "outputId": "1ea132f4-e550-4b51-f0c6-7f3551daafb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  5 105 526 ...   1   1   1]\n"
          ]
        }
      ],
      "source": [
        "# the i^th entry tells us how many\n",
        "# individual documents the i^th word in the dictionary appeared in\n",
        "dfArray = np.array(docIDAndPos.distinct()\\\n",
        "                   .groupBy('dictNum')\\\n",
        "                   .agg(f.count('docID'))\\\n",
        "                   .drop('dictNum')\n",
        "                   .collect()).flatten()\n",
        "print(dfArray)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYdsW30rHFkv"
      },
      "outputs": [],
      "source": [
        "# Get the version of dfArray where the i^th entry is the inverse-document frequency for the\n",
        "# i^th word in the corpus\n",
        "numberOfDocs = wikiPages.count()\n",
        "idfArray = np.log(np.divide(np.full(20000, numberOfDocs), dfArray))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hat-C0_-WcpR"
      },
      "outputs": [],
      "source": [
        "multiply_idfArray = udf(lambda x: np.multiply(x, idfArray).tolist(), ArrayType(FloatType(), containsNull=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glXJGWulR3yk",
        "outputId": "93f27b16-9c87-4f6a-ef79-dac2cf7f8a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+\n",
            "| docID|               TFidf|\n",
            "+------+--------------------+\n",
            "|434061|[1.6173673E-5, 6....|\n",
            "|455037|[0.0, 0.0, 0.0, 0...|\n",
            "+------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Finally, convert all of the tf vectors in allDocsAsNumpyArrays to tf * idf vectors and remove []\n",
        "# Then split by ,\n",
        "allDocsAsNumpyArraysTFidf = tfArrays.withColumn('TFidf', multiply_idfArray('tf'))\\\n",
        ".drop('tf')\n",
        "\n",
        "print(allDocsAsNumpyArraysTFidf.show(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XMY_fmrXsVT"
      },
      "outputs": [],
      "source": [
        "allDocsAsNumpyArraysTFidf.write\\\n",
        ".mode(\"overwrite\")\\\n",
        ".option(\"header\",True)\\\n",
        ".parquet(project_folder + \"/df_results/task2_tfidf/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2hJPDsOXzep"
      },
      "source": [
        "### Task 3 - Implement the getPrediction function (30 Points)\n",
        "Finally, implement the function getPrediction(textInput, k), which will predict the\n",
        "membership of the textInput to the top 20 closest documents, and the list of top\n",
        "categories.\n",
        "You should use the cosine similarity to calculate the distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flzU9sulR3l0",
        "outputId": "7bd1ba4d-3d50-4bab-d878-ed6f5f5c758d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+\n",
            "|   _c0|                 _c1|\n",
            "+------+--------------------+\n",
            "|434042|   1987_debut_albums|\n",
            "|434042|Albums_produced_b...|\n",
            "+------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "wikiCategoryFile = project_folder + \"wiki-categorylinks-small.csv.bz2\"\n",
        "\n",
        "wikiCats=spark.read.format('csv')\\\n",
        ".options(header = 'false', inferSchema = 'true', sep = ',')\\\n",
        ".load(wikiCategoryFile)\n",
        "wikiCats.show(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6gcN0-lV3Ue",
        "outputId": "5598ed08-fbae-41b5-f99a-63085c8af3c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|            category|               TFidf|\n",
            "+--------------------+--------------------+\n",
            "|Use_dmy_dates_fro...|[1.6173673E-5, 6....|\n",
            "|Politics_of_East_...|[1.6173673E-5, 6....|\n",
            "|Lists_of_politica...|[1.6173673E-5, 6....|\n",
            "|Leaders_of_East_G...|[1.6173673E-5, 6....|\n",
            "|East_Germany_poli...|[1.6173673E-5, 6....|\n",
            "|Articles_lacking_...|[1.6173673E-5, 6....|\n",
            "|Articles_containi...|[1.6173673E-5, 6....|\n",
            "|All_articles_lack...|[1.6173673E-5, 6....|\n",
            "|Human_name_disamb...|[0.0, 0.0, 0.0, 0...|\n",
            "|Disambiguation_pa...|[0.0, 0.0, 0.0, 0...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now, we join it with categories, and map it after join so that we have only the wikipageID \n",
        "# This joun can take time on your laptop. \n",
        "# You can do the join once and generate a new wikiCats data and store it. Our WikiCategories includes all categories\n",
        "# of wikipedia. \n",
        "\n",
        "wikiAndCatsJoind = wikiCats.withColumnRenamed('_c0', 'docID')\\\n",
        "            .withColumnRenamed('_c1', 'category')\\\n",
        "            .join(allDocsAsNumpyArraysTFidf, on='docID')\n",
        "featuresDF = wikiAndCatsJoind.select('category', 'TFidf')\n",
        "\n",
        "# Cache this important data because we need to run kNN on this data set. \n",
        "featuresDF.cache()\n",
        "featuresDF.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1eMy63MR3pL"
      },
      "outputs": [],
      "source": [
        "def cosineSim (x,y):\n",
        "\tnormA = np.linalg.norm(x)\n",
        "\tnormB = np.linalg.norm(y)\n",
        "\treturn (np.dot(x,y)/(normA*normB)).tolist()\n",
        "\n",
        "cosinSim_udf = udf(cosineSim, FloatType())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elU_EIt3VivE"
      },
      "outputs": [],
      "source": [
        "# Assumption: Each document is stored in one line of the text file\n",
        "# We need this count later ... \n",
        "numberOfDocs = wikiPages.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKN9YQyXYLkS"
      },
      "outputs": [],
      "source": [
        "def getPrediction(textInput,k):\n",
        "    # create a df \n",
        "    df = spark.createDataFrame([textInput], StringType())\n",
        "    print(df.show(2))\n",
        "    #Flat map the text to (word, 1) pair for each word in the doc\n",
        "    textWords =df.withColumn ('word', f.explode(f.split(f.lower(\n",
        "                                                  f.regexp_replace('value', '[^a-zA-Z]', ' ')), ' '))).\\\n",
        "                                                  withColumn('count', f.lit(1))\\\n",
        "                                                  .filter('word != \"\"')\\\n",
        "                                                  .drop('value')\n",
        "    print(textWords.show(2))      \n",
        "    # This will give us a set of (word, (dictionaryPos, 1)) pairs\n",
        "    allDictionaryWordsInThatDoc = dictionary.join (textWords, on='word').\\\n",
        "                                  select('dictNum', 'count').groupBy('count').\\\n",
        "                                      agg(f.collect_set('dictNum'))\n",
        "    print(allDictionaryWordsInThatDoc.show(2))\n",
        "    #Get tf array for the input string\n",
        "    tfArray = allDictionaryWordsInThatDoc.orderBy('count', ascending = False).limit(1)\\\n",
        "                          .withColumn('tfArray', tfUDF('collect_set(dictNum)')).\\\n",
        "                          select('tfArray') \n",
        "    print(tfArray.show(2))    \n",
        "    # Multiply by idfArray\n",
        "    myArray = tfArray.withColumn('tfxIdf', multiply_idfArray('tfArray')).select('tfxIdf')\n",
        "    print(myArray.show(2))    \n",
        "    \n",
        "    # Get the tf * idf array for the input string\n",
        "    # Get the distance from the input text string to all database documents, \n",
        "    # using cosine similarity (np.dot() )\n",
        "    distances = featuresDF.join(myArray)\n",
        "    distances = distances.withColumn('distances', cosinSim_udf('TFidf', 'tfxIdf'))\\\n",
        "                            .select('category', 'distances')\n",
        "    print(distances.show(2))\n",
        "    \n",
        "    # get the top k distances\n",
        "    topK = distances.orderBy('distances', ascending = False).limit(k)\n",
        "    print(topK.show(5))\n",
        "\n",
        "    # now, for each docID, get the count of the number of times this document ID appeared in the top k\n",
        "    numTimes = topK.groupBy('category')\\\n",
        "                  .agg(f.count('category').alias('count'))\\\n",
        "                  .drop('distances')\n",
        "    print(numTimes.show(5))\n",
        "\n",
        "\n",
        "    # Return the top 1 of them.\n",
        "    # Ask yourself: Why we are using twice top() operation here?\n",
        "    # Answer: to show them in sorted order\n",
        "    return numTimes.orderBy('count', ascending = False).limit(k).collect()                                                 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eBcfifzYLg9"
      },
      "outputs": [],
      "source": [
        "# textInput = \"Big data refers to data sets that are too large or complex to be\"\n",
        "# dealt with by traditional data-processing application software. Data with many \\\n",
        "# fields (rows) offer greater statistical power, while data with higher complexity\\\n",
        "#  (more attributes or columns) may lead to a higher false discovery rate.[2] \\\n",
        "#  Big data analysis challenges include capturing data, data storage, data \\\n",
        "#  analysis, search, sharing, transfer, visualization, querying, updating, \\\n",
        "#  information privacy, and data source. Big data was originally associated with \\\n",
        "#  three key concepts: volume, variety, and velocity.[3] The analysis of big data \\\n",
        "#  presents challenges in sampling, and thus previously allowing for only \\\n",
        "#  observations and sampling. Thus a fourth concept, veracity, refers to the \\\n",
        "#  quality or insightfulness of the data. Without sufficient investment in \\\n",
        "#  expertise for big data veracity, then the volume and variety of data can \\\n",
        "#  produce costs and risks that exceed an organization's capacity to create \\\n",
        "#  and capture value from big data.[4]\"\n",
        "\n",
        "# getPrediction(textInput, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEV4WDWbYoHz",
        "outputId": "0f2323a8-9118-4830-c1bc-dd316587ea83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|               value|\n",
            "+--------------------+\n",
            "|How many goals Va...|\n",
            "+--------------------+\n",
            "\n",
            "None\n",
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| how|    1|\n",
            "|many|    1|\n",
            "+----+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n",
            "+-----+--------------------+\n",
            "|count|collect_set(dictNum)|\n",
            "+-----+--------------------+\n",
            "|    1|[66, 63, 2626, 64...|\n",
            "+-----+--------------------+\n",
            "\n",
            "None\n",
            "+--------------------+\n",
            "|             tfArray|\n",
            "+--------------------+\n",
            "|[0.0, 0.0, 0.0, 0...|\n",
            "+--------------------+\n",
            "\n",
            "None\n",
            "+--------------------+\n",
            "|              tfxIdf|\n",
            "+--------------------+\n",
            "|[0.0, 0.0, 0.0, 0...|\n",
            "+--------------------+\n",
            "\n",
            "None\n",
            "+--------------------+----------+\n",
            "|            category| distances|\n",
            "+--------------------+----------+\n",
            "|Use_dmy_dates_fro...|0.03551864|\n",
            "|Politics_of_East_...|0.03551864|\n",
            "+--------------------+----------+\n",
            "only showing top 2 rows\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(getPrediction('How many goals Vancouver score last year?', 10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WGD3dU5sy1Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyOpf5ldYkp4Cz9XiNujZK1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}