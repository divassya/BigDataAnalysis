{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcXgSWdMrNPYZHcoU6GGJf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divassya/BigDataAnalysis/blob/main/AssiyaKaratay_Assignment_4_Wiki_Categories_SparkRDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Info \n",
        "Assignment 4\n",
        "MET CS777 Big Data Analytics\n",
        "\n",
        "Faculty - Farshid Alizadeh-Shabdiz, PhD, MBA\n",
        "\n",
        "Student - Assiya Karatay U95161396 karatay@bu.edu 857-294-7028"
      ],
      "metadata": {
        "id": "Htig3sG60x46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### import libraries"
      ],
      "metadata": {
        "id": "ezcf-q2j1KTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muck9GEM0tKR",
        "outputId": "0a7c4ff1-dad7-4944-90ee-33fa243adcf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 65 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 58.8 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --ignore-installed -q pyspark==3.1.2 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import requests\n",
        "import numpy as np\n",
        "from operator import add\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "import re\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "PR7PUYMr0175"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### set up the Google Drive"
      ],
      "metadata": {
        "id": "8uRljevs1Zf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### set up the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f2nVFna1VTa",
        "outputId": "b341affe-f61f-44a5-ee8c-577d4c196405"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choose where project files will be saved\n",
        "project_folder = \"/content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\"\n",
        "# project_folder = sys.argv[2]\n",
        "# change the OS to use the project folder as the working directory\n",
        "os.chdir(project_folder)\n",
        "\n",
        "print('\\n Working directory was changed to ' + project_folder )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGMtloP1X2W",
        "outputId": "1e08adc6-69e4-4c86-e958-e68e8a4dcc76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Working directory was changed to /content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1 Generate a 20K dictionary (10 points)"
      ],
      "metadata": {
        "id": "_YBqW38I7c3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 The top 20,000 English words\n",
        "Using Wikipedia pages, find the top 20,000 English words, save them in an array, and sort them based on the frequency of the occurrence."
      ],
      "metadata": {
        "id": "45QO8PUsad19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parsing(lines):\n",
        "  # divide the data defore and after url\n",
        "  division = lines.split('\" url')\n",
        "  # strip all characters except DOC ID\n",
        "  id = division[0].split('<doc id=\"')[1] \n",
        "  # strip the end of title and last 6 characters containing '.</doc' .\n",
        "  text = division[1].split('\">')[1][:-6]\n",
        "  # check the UNICODE decoding for readability \n",
        "  # and lowercase to count words with the same letters together\n",
        "  regex = re.compile(r'[^a-zA-Z]', re.UNICODE).split(text.lower())\n",
        "  return (id,regex)"
      ],
      "metadata": {
        "id": "Hd7UWgpxHGfa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. get the file\n",
        "wikiPagesFile= project_folder + \"WikipediaPagesOneDocPerLine1000LinesSmall.txt.bz2\"\n",
        "# 2. create an rdd of data\n",
        "wikiPages = sc.textFile(wikiPagesFile)\n",
        "# 3. check if each line has id and url\n",
        "validLines = wikiPages.filter(lambda x : 'id' in x and 'url=' in x)\n",
        "# 4. trim unnecessary symbols and get (docID, listOfWords) pairs\n",
        "listOfWords = validLines.map(parsing)\n",
        "# 5. make every word a tuple dropping DocID\n",
        "wordsAsTuples = listOfWords.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
        "# 6. count number of words in a corpus\n",
        "counts=wordsAsTuples.reduceByKey(lambda x, y: x+y)\n",
        "# 7. get the top 20K in a corpus\n",
        "n = 20000\n",
        "topWords = counts.top(n, lambda x: x[1])\n",
        "# 8. create an empty RDD that has the number 0 through 20000\n",
        "emptyRDD = sc.parallelize(range(n))\n",
        "# 9. order words in descending popularity order, where  \n",
        "# the word in position 0 is the most common used word\n",
        "dictionary = emptyRDD.map (lambda x : (topWords[x][0], x))\n",
        "# 10. save the output \n",
        "dictionary.saveAsTextFile(project_folder+'task11_topEnglishWords')\n",
        "print(\"The top 20,000 English words\", dictionary.top(5, lambda x : -x[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUIJWH6ZFryD",
        "outputId": "8ebd3fb6-ba2a-4bcc-8e94-d06d7cfcbacc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top 20,000 English words [('', 0), ('the', 1), ('of', 2), ('and', 3), ('in', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 docID as key and a Numpy array for the position of each word\n",
        "As a result, a dictionary has been generated that contains the top 20K most frequent words in the corpus. Next go over each Wikipedia document and check if the words appear in the Top 20K words. At the end, produce an RDD that includes\n",
        "the docID as key and a Numpy array for the position of each word in the top 20K dictionary.\n",
        "(docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...])"
      ],
      "metadata": {
        "id": "MaiOq8LFaWjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numberOfOccurrences(docIDAndPos):\n",
        "    docID = docIDAndPos[0]\n",
        "    listOfIndices = docIDAndPos[1]\n",
        "    # create an array of zeros\n",
        "    returnVal = np.zeros(20000)\n",
        "    # count the occurrence of words e.g. there are 514 'my' in docID1, where \n",
        "    # 'my' is in the position 0 in the corpus\n",
        "    for index in listOfIndices:\n",
        "        returnVal[index] = returnVal[index] + 1\n",
        "    return (docID, returnVal)"
      ],
      "metadata": {
        "id": "Mlz9BDSUVxzU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. get (\"word1\", docID) pairs from (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
        "allWordsWithDocID = listOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
        "# 2. inner join with dictionary to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
        "allDictionaryWords = dictionary.join(allWordsWithDocID)\n",
        "# 3. drop the word as string to get a set of (docID, dictionaryPos) pairs\n",
        "docIDAndPos = allDictionaryWords.map(lambda x: (x[1][1], x[1][0]))\n",
        "# 4. get a pair (docID, np.array('count of word in pos1', 'count of word in pos2',...))\n",
        "allDictionaryWordsInDocID = docIDAndPos.groupByKey()\n",
        "IDPosArray = allDictionaryWordsInDocID.map(numberOfOccurrences)\n",
        "# 5. save the output\n",
        "IDPosArray.saveAsTextFile(project_folder+'task12_wordOccurrences')"
      ],
      "metadata": {
        "id": "gx4YALyzRVoa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2 - Create the TF-IDF Array (20 Points)\n",
        "#### TF\n",
        "After having the top 20K words we want to create a large array that its\n",
        "columns are the words of the dictionary with number of occurrences of each word and the rows are documents.\n",
        "The first step is calculating the “Term Frequency”, TF (x, w), vector for each document as follows:\n",
        "“Term Frequency” is an indication of the number of times a term occurs in a document.\n",
        "Numerator is number of occurrences of a word, and the denominator is the sum of all the words of the document."
      ],
      "metadata": {
        "id": "mbSuv2tWVyWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def termFrequency(IDPosArray):\n",
        "    numberOfWords = np.sum(IDPosArray[1])\n",
        "    returnVal = np.divide(IDPosArray[1], numberOfWords)\n",
        "    return (IDPosArray[0], returnVal)\n",
        "# Now get (docID, [TF1, TF2, ...]) pairs\n",
        "tf = IDPosArray.map(termFrequency)"
      ],
      "metadata": {
        "id": "EoCukGAJnV1c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IDF\n",
        "Next, calculate “Inverse Document Frequency” for all the documents and finally\n",
        "calculate TF-IDF(w) and create TF-IDF matrix of the corpus:\n",
        "Note that the “size of corpus” is total number of documents (numerator).\n",
        "To learn more about TF-IDF see the Wikipedia page:\n",
        "https://en.wikipedia.org/wiki/Tf-idf"
      ],
      "metadata": {
        "id": "-6x5eGRLeVAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def occurrenceSign(tf):\n",
        "    # empty np array of size 20K\n",
        "    returnVal = np.zeros (20000)\n",
        "    # get positions in dictionary of which words occurred in a doc\n",
        "    tfZero = np.where(tf[1]>0)[0]\n",
        "    for index in tfZero:\n",
        "        if returnVal[index] == 0: \n",
        "          returnVal[index] = 1\n",
        "    # A zero means that the word does not occur,one means that it does.\n",
        "    return (tf[0], returnVal)"
      ],
      "metadata": {
        "id": "Erd_JAqcwklK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. build occurrence table of words as 1 and 0\n",
        "zeroOrOne = tf.map(occurrenceSign)\n",
        "# 2. count the number of documents where each word in a dict appeared\n",
        "dfArray = zeroOrOne.values().sum()\n",
        "# 3. create an array of numberOfDocs of size 20K\n",
        "numberOfDocs = wikiPages.count()\n",
        "multiplier = np.full(n, numberOfDocs)\n",
        "# 4. get the version of dfArray where the i^th entry is the idf for the i^th word in the corpus\n",
        "idfArray = np.log(np.divide(multiplier, dfArray))\n",
        "# 5. convert all of the tf vectors to tf*idf vectors\n",
        "tfidf = tf.map(lambda x: (x[0], np.multiply(x[1], idfArray)))\n",
        "# 6. save the output\n",
        "while True:\n",
        "      try:  \n",
        "        tfidf.saveAsTextFile(project_folder+'task2_tfidf')        \n",
        "        break\n",
        "      except:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "kYCKvwvKlGav"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3 - Implement the getPrediction function (30 Points)\n",
        "Finally, implement the function getPrediction(textInput, k), which will predict the\n",
        "membership of the textInput to the top 20 closest documents, and the list of top\n",
        "categories.\n",
        "You should use the cosine similarity to calculate the distances."
      ],
      "metadata": {
        "id": "hwojfp4ewFOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parsingCategories(lines):\n",
        "  # separate the data before and after comma\n",
        "  division = lines.replace('\"', '').split(',')\n",
        "  return (division[0],division[1])"
      ],
      "metadata": {
        "id": "7gl2hZ_Ownbm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that returns the prediction for the label of a string, using a kNN algorithm\n",
        "def getPrediction (textInput, k):\n",
        "    # 1. create an rdd of the text input\n",
        "    myDoc = sc.parallelize (('', textInput))\n",
        "    # 2. get (word, 1) pairs and preprocess the data\n",
        "    textWords = myDoc.flatMap (lambda x : ((j, 1) for j in re.compile(r'[^a-zA-Z]', re.UNICODE).sub(' ', x).lower().split()))\n",
        "    # 3. join dict(word, pos) with text words(word,1) to get (word, (dictionaryPos, 1)) pairs\n",
        "    myDocAndDictJoined = dictionary.join (textWords)\n",
        "    # 4. drop the string word and have (1,dictionaryPos)\n",
        "    allDictionaryWordsInThatDoc = myDocAndDictJoined.map (lambda x: (x[1][1], x[1][0])).groupByKey()  \n",
        "    # 5. array of occurrences (1, np.array(0, 2, 5, ...))\n",
        "    IDPosArray = allDictionaryWordsInThatDoc.map(numberOfOccurrences)\n",
        "    # 6. Get tf array for the text input \n",
        "    tf = IDPosArray.map(termFrequency).collect()[0][1]\n",
        "    # 7. get tfidf array \n",
        "    myArrayTfidf = np.multiply (tf, idfArray)\n",
        "    # 8. measure distance from the text tfidf to all cats tfidf, using cosine similarity (np.dot() )\n",
        "    distances = featuresRDD.map (lambda x : (x[0], np.dot (x[1], myArrayTfidf)))\n",
        "    # 9. get the top k largest distances (cos0 = 1 means similarity, cos90 = 0 means difference)\n",
        "    topK = distances.top (k, lambda x : x[1])\n",
        "    # 10. transform the top k distances into (cat_name, 1) pairs\n",
        "    docIDRepresented = sc.parallelize(topK).map (lambda x : (x[0], 1))\n",
        "    # 11. for each cat_name, count the number of times this docID appeared in the top k\n",
        "    numTimes = docIDRepresented.reduceByKey(add)\n",
        "    # 12. Return the most occurred cat_names in the closest cat_names \n",
        "    topNumTimes = numTimes.top(k, lambda x: x[1])\n",
        "    while True:\n",
        "      try:  \n",
        "        sc.parallelize(topNumTimes).saveAsTextFile(project_folder + 'task3_topCategories'+str(textInput[:6]))\n",
        "        break\n",
        "      except:\n",
        "        break\n",
        "    return topNumTimes"
      ],
      "metadata": {
        "id": "CXWfTPXqwnXP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. get categories file\n",
        "wikiCategoryFile=project_folder + \"wiki-categorylinks-small.csv.bz2\"\n",
        "# wikiCategoryFile = sys.argv[2]\n",
        "# 2. create an rdd\n",
        "wikiCategoryLinks=sc.textFile(wikiCategoryFile)\n",
        "# 3. delete \" and get (docID,cat_name) pairs\n",
        "wikiCats = wikiCategoryLinks.map(parsingCategories)\n",
        "# 4. join tfidf (docID, tdidf array) with categories (docID,cat_name)\n",
        "catsTFIDFJoined = wikiCats.join(tfidf)\n",
        "# 5. drop docID and obtain category name and tfidf array\n",
        "featuresRDD = catsTFIDFJoined.map(lambda x: (x[1][0], x[1][1]))\n",
        "# 6. Cache because kNN will be run on this data \n",
        "featuresRDD.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkZoxk4QjwRo",
        "outputId": "c2c0c583-7fa6-4a0f-c99d-66fa782be827"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('All_articles_with_unsourced_statements', array([0.        , 0.00404342, 0.00183544, ..., 0.        , 0.        ,\n",
            "       0.        ]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textInput = \"Big data refers to data sets that are too large or complex to be \\\n",
        "dealt with by traditional data-processing application software. Data with many \\\n",
        "fields (rows) offer greater statistical power, while data with higher complexity\\\n",
        " (more attributes or columns) may lead to a higher false discovery rate.[2] \\\n",
        " Big data analysis challenges include capturing data, data storage, data \\\n",
        " analysis, search, sharing, transfer, visualization, querying, updating, \\\n",
        " information privacy, and data source. Big data was originally associated with \\\n",
        " three key concepts: volume, variety, and velocity.[3] The analysis of big data \\\n",
        " presents challenges in sampling, and thus previously allowing for only \\\n",
        " observations and sampling. Thus a fourth concept, veracity, refers to the \\\n",
        " quality or insightfulness of the data. Without sufficient investment in \\\n",
        " expertise for big data veracity, then the volume and variety of data can \\\n",
        " produce costs and risks that exceed an organization's capacity to create \\\n",
        " and capture value from big data.[4]\"\n",
        "\n",
        "getPrediction(textInput, 20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUeRB9TIkGzu",
        "outputId": "75fdc185-fde0-4516-e3e9-ab004c546bfb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('All_stub_articles', 3),\n",
              " ('Programming_language_topic_stubs', 2),\n",
              " ('Data_modeling_languages', 2),\n",
              " ('XML-based_standards', 2),\n",
              " ('All_Wikipedia_articles_needing_context', 1),\n",
              " ('All_articles_lacking_sources', 1),\n",
              " ('All_pages_needing_cleanup', 1),\n",
              " ('Articles_with_multiple_maintenance_issues', 1),\n",
              " ('Computer_networks', 1),\n",
              " ('Computing_stubs', 1),\n",
              " ('Use_dmy_dates_from_May_2019', 1),\n",
              " ('All_articles_needing_additional_references', 1),\n",
              " ('Articles_needing_additional_references_from_May_2016', 1),\n",
              " ('Articles_lacking_sources_from_December_2009', 1),\n",
              " ('Wikipedia_articles_needing_context_from_October_2009', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4 – Implement the code using Dataframes (30 points)\n",
        "Implement the complete code in Dataframe and print out the results of the task 3\n",
        "using dataframes in pyspark. From the beginning of your code to the end of your\n",
        "kNN implementation you are allowed to use spark dataframe and python (including\n",
        "python libraries like numpy). You are not allowed to use RDDs."
      ],
      "metadata": {
        "id": "8bbB-O7K7jQs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkWbI2O27i5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GYZskPv7i8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VT_pbBmU7i_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5 - Removing Stop Words and Do Stemming (10 points)\n",
        "Task 5.1 - Remove Stop Words (5 point)\n",
        "Describe if removing the English Stop words (most common words like ”a,\n",
        "the, is, are, i, you, ...”) would change the final kNN results.\n",
        "Does your result change significantly after removing the stop words? Why?\n",
        "Provide reasons.\n",
        "You do not need to implement this task.\n",
        "Task 5.2 – Considering English word stemming (5 point)\n",
        "We can stem the words [”game”,”gaming”,”gamed”,”games”] to their root\n",
        "word ”game”.\n",
        "Does stemming change your result significantly? Why? Provide reasons.\n",
        "You can learn more about stemming at:\n",
        "https://en.wikipedia.org/wiki/Stemming\n",
        "You do not need to implement this task."
      ],
      "metadata": {
        "id": "gCnlRk7eNBYX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFmIaC6e7tjq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}