{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divassya/BigDataAnalysis/blob/main/AssiyaKaratay_Assignment_2_Grades_PiEstimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OqQtFjXodxe"
      },
      "source": [
        "### Assignment 2\n",
        "MET CS777 Big Data Analytics \n",
        "\n",
        "\n",
        "Faculty - Farshid Alizadeh-Shabdiz, PhD, MBA\n",
        "\n",
        "Student - Assiya Karatay U95161396 karatay@bu.edu 857-294-7028"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmYWVtZvodxg"
      },
      "source": [
        "\n",
        "### Problem 1 (10 points)\n",
        "\n",
        "What are the differences between following RDD operations from functionality’s perspective, and also computation and cost of operation.\n",
        "Assign a level of computation complexity from level 1 (less costly) to level 3 (most costly) to each one. \n",
        "\n",
        "P.S. I added documentation for my future use.\n",
        "\n",
        "#### 1.\taggregateByKey() - level 2 \n",
        "- From documentation: Aggregate the values of each key, using given combine functions and a neutral “zero value”. This function can return a different result type, U, than the type of the values in this RDD, V. \n",
        "\n",
        "This is the same as reduceByKey with addition of an initial value. This method is more efficient than groupByKey operation.\n",
        "\n",
        "\n",
        "#### 2.\treduceByKey() - level 2 \n",
        "- from documentation \"\"\"\n",
        "        Merge the values for each key using an associative and commutative reduce function.\n",
        "\n",
        "        This will also perform the merging locally on each mapper before\n",
        "        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
        "\n",
        "        Output will be partitioned with `numPartitions` partitions, or\n",
        "        the default parallelism level if `numPartitions` is not specified.\n",
        "        Default partitioner is hash-partition.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> from operator import add\n",
        "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "        >>> sorted(rdd.reduceByKey(add).collect())\n",
        "        [('a', 2), ('b', 1)]\n",
        "        \"\"\"\n",
        "What I understand is that there are partitions. Then data is collected like shuffling. There is only one output for one key at each partition to send over the network. This method is more efficient than groupByKey operation.\n",
        "\n",
        "#### 3.\tgroupByKey() - level 3 - from documentation: Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions.\n",
        "\n",
        "Note:If we are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will provide much better performance\n",
        "\n",
        " What I understand is that groupByKey's memory usage is huge, so it can cause out of disk problems as data is sent over the network and collected on the reduced workers. This function is a little bit inefficient, but it can be used when necessary. \n",
        "\n",
        "#### 4.\tcombineByKey() - level 1 \n",
        "- From documentation  \"\"\"\n",
        "        Generic function to combine the elements for each key using a custom\n",
        "        set of aggregation functions.\n",
        "\n",
        "        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
        "        type\" C.\n",
        "\n",
        "        Users provide three functions:\n",
        "\n",
        "            - `createCombiner`, which turns a V into a C (e.g., creates\n",
        "              a one-element list)\n",
        "            - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n",
        "              a list)\n",
        "            - `mergeCombiners`, to combine two C's into a single one (e.g., merges\n",
        "              the lists)\n",
        "\n",
        "        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
        "        modify and return their first argument instead of creating a new C.\n",
        "\n",
        "        In addition, users can control the partitioning of the output RDD.\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        V and C can be different -- for example, one might group an RDD of type\n",
        "            (Int, Int) into an RDD of type (Int, List[Int]).\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
        "        >>> def to_list(a):\n",
        "        ...     return [a]\n",
        "        ...\n",
        "        >>> def append(a, b):\n",
        "        ...     a.append(b)\n",
        "        ...     return a\n",
        "        ...\n",
        "        >>> def extend(a, b):\n",
        "        ...     a.extend(b)\n",
        "        ...     return a\n",
        "        ...\n",
        "        >>> sorted(x.combineByKey(to_list, append, extend).collect())\n",
        "        [('a', [1, 2]), ('b', [1])]\n",
        "        \"\"\"\n",
        "\n",
        "This is more complicated, but provides more room for wrangling inputs. Three parameters are passed as input such as an initial value, merging and combine function. I does not need to pass constants always, a function can be passed too that will return a new value. This is the least costly operation among this set. \n",
        "\n",
        "### Problem 2 (5 points)\n",
        "Name at least four differences between Spark and Hadoop MapReduce?\n",
        "• Speed \n",
        "- Batch Processing is not sutable for some business cases\n",
        "- Spark builds its query computations as a directed acyclic graph (DAG); its DAG scheduler and query optimizer construct an efficient computational graph that can usually be decomposed into tasks that are executed in parallel across workers on the cluster. Its physical execution engine, Tungsten, uses whole-stage code generation to generate compact code for execution (from slides). This means that Spark has better speed on processing than MapReduce. \n",
        "• Ease of use\n",
        "- MapReduce needs Java which is not as user-friendly as other languages\n",
        "- Spark has simple logical data structure, RDD at the core\n",
        "• Modularity\n",
        "- Spark supports most programming languages: Scala, Java, Python, SQL,and R and Components like: Spark SQL, Spark ML, Spark Streaming, and Graphx\n",
        "• Extensibility\n",
        "- Spark decouples storage and compute, unlike Hadoop.\n",
        "- Read and write from many sources like Apache Kafka, Kinesis, Azure storage, AWS S3, etc.\n",
        "\n",
        "### Problem 3 (5 points)\n",
        "\n",
        "How does Spark run an application and what are the driver layer functionalities? Explain from the Spark architecture’s perspective. \n",
        " - The driver layer two main functionalities are converting user programs into tasks and planning the execution of tasks by executors. In details, \n",
        "* it runs the main() function of the application \n",
        "* it schedules the job execution.\n",
        "* Spark Context and RDDs are created there, and their metadata is saved there\n",
        "* It translates the RDD’s into the execution graph and splits the graph into multiple stages.\n",
        "* It converts the application into smaller execution tasks. Tasks are then executed by the executors. After the task has been completed, all the executors submit their results to the Driver.\n",
        "* It exposes the information about all the process through a Web UI\n",
        "\n",
        "### Problem 4 (5 points)\n",
        "\n",
        "What are the differences of running on multi-core computer versus running on multi worker/executor environment? \n",
        "List pros and cons of each one.\n",
        "\n",
        "* Multi-core computer means a computer with more than one processing units. It can be used for multi-threading of the task with shared memory like hard drive, RAM, and cache. \n",
        "\n",
        "* Multi worker environment shares nothing in terms of data, processes,connection between machines. Multi worker environment can have (and usually is better to have) multi-core computers. It is much more efficient than one multi-core computer when the data is large (gigabytes, petabytes).\n",
        " \n",
        "### Problem 5 (10 points)\n",
        "\n",
        "Why RDD is immutable? Was this a mistake in design of RDDs or has some advantage?\n",
        "RRD's immutability means our code does not change when we copy objects in RDD. It has many benefits:\n",
        "1. easy to parallelize: When we use  parts of the data in workers, we are certain that the data at the source is not modified. \n",
        "2. This prevents from a set of potential problems due to updates from multiple threads at once. Immutable data is definitely safe to share across processes.\n",
        "3. the RDD's parts can be recreated at any time. This makes caching, sharing and replication easy.\n",
        "4. gaining the fault tolerance and correctness with no developer effort is worth spending memory and CPU on, since the latter are cheap.\n",
        "\n",
        "### Problem 6 (10 points)\n",
        "\n",
        "Spark transformation is divided to narrow transformation and wide transformation. Referring to spark documentation, explain the differences.\n",
        "\n",
        "* Narrow transformations are processed on a single partition with no data movement between partitions .Examples: map(), mapPartition(), flatMap(), filter(), union() \n",
        "* Wide transformations are processed on many partition swith data movements between partitions. It is expensive due to shuffling. Examples:groupByKey(), aggregateByKey(), aggregate(), join(), repartition() \n",
        "\n",
        "### Problem 7 (15 points)\n",
        "\n",
        "List 10 spark transformation operation with one line of example\n",
        "1. rdd.map(x=>x+2) on RDD {1, 2, 3, 4, 5} applied to every element returns only one element (3, 4, 5, 6, 7).\n",
        "2. rdd.flatMap(lines => lines.split(” “)) splits each input string into words when space occurs. flatMap() can return a list of elements.\n",
        "3. filter() function returns a new RDD, containing only the elements that meet a condition. rdd.filter(value => value%2==0) returns even numbers.\n",
        "4. union() function gives the elements of both the same type RDD in new RDD. RDD1 = (Spark, Hadoop, Flink) and RDD2 = (Big data, Spark, Flink), so the resultant rdd1.union(rdd2) will be (Spark, Spark, Hadoop, Flink, Flink, Big data).\n",
        "5. intersection() gives the common element in both RDDs (Spark, Spark, Hadoop, Flink) and (Big data, Spark, Flink), so the resultant rdd1.intersection(rdd2) will have elements (spark, flink).\n",
        "6. rdd.distinct() is helpful to remove duplicate data  and gives elements (Spark, Hadoop, Flink) from (Spark, Spark, Hadoop, Flink).\n",
        "7. groupByKey() shuffles the data according to the key value in another RDD: spark.sparkContext.parallelize(Array((‘k’,5),(‘s’,3),(‘s’,4),(‘p’,7),(‘p’,5),(‘t’,8),(‘k’,6)),3) group = data.groupByKey().collect()\n",
        "8. sortByKey() sorts the data according to the key in another RDD: data = spark.sparkContext.parallelize(Seq((“maths”,52), (“english”,75), (“science”,82), (“computer”,65), (“maths”,85))) sorted = data.sortByKey() sorts the data RDD into Ascending order of the Key(String).\n",
        "9. join() operation combines two data sets on the basis of the key.\n",
        "data = spark.sparkContext.parallelize(Array((‘A’,1),(‘b’,2),(‘c’,3)))\n",
        "data2 =spark.sparkContext.parallelize(Array((‘A’,4),(‘A’,6),(‘b’,7),(‘c’,3),(‘c’,8)))\n",
        "result = data.join(data2)\n",
        "10. coalesce() avoids full shuffling of data and uses existing partition so that less data is shuffled. Using this we can cut the number of the partition, e.g. only two nodes out of 4. \n",
        "rdd1 = spark.sparkContext.parallelize(Array(“jan”,”feb”,”mar”,”april”,”may”,”jun”),3)\n",
        "val result = rdd1.coalesce(2)\n",
        "\n",
        "List 5 spark action operation with one line example.\n",
        "1. rdd.count() returns the number of elements 8 from rdd={1, 2, 2, 3, 4, 5, 5, 6}\n",
        "    countByValue() returns how many times each element occur {(1,1), (2,2), (3,1), (4,1), (5,2), (6,1)} in RDD = {1, 2, 2, 3, 4, 5, 5, 6}   \n",
        "2. result.collect() returns our entire RDDs content to driver program. The application of collect() is unit testing where the entire RDD is expected to fit in memory. It has a constraint that all the data should fit in the machine, and copies to the driver.\n",
        "3.  result.take(n) returns n number of elements as a biased collection from RDD.\n",
        "4. fold() is like reduce(). Besides, it takes “zero value” as input, which is used for the initial call on each partition. But, the condition with zero value is that it should be the identity element of that operation. The key difference between fold() and reduce() is that, reduce() throws an exception for empty collection, but fold() is defined for empty collection. For example, zero is an identity for addition; one is identity element for multiplication. \n",
        "additionalMarks = (“extra”, 4)\n",
        "sum = rdd1.fold(additionalMarks){ (acc, marks) => val add = acc._2 + marks._2\n",
        "Note – additionalMarks is an initial value. This value will be added to the int value of each record in the source RDD.\n",
        "5. foreach() applies operation on each element of RDD, but it does not return value to the driver. For example, inserting a record into the database.\n",
        "data = spark.sparkContext.parallelize(Array((‘k’,5),(‘s’,3),(‘s’,4),(‘p’,7),(‘p’,5),(‘t’,8),(‘k’,6)),3)\n",
        "group = data.groupByKey().collect()\n",
        "group.foreach(println) # run a function (println) on each element of the dataset group.\n",
        "\n",
        "### Problem 8 (20 points)\n",
        "\n",
        "We have a data which consists of following columns\n",
        "-\tRow number\n",
        "-\tFirst name of student\n",
        "-\tLast name of student\n",
        "-\tCourse number\n",
        "-\tGrade \n",
        "\n",
        "Write an efficient Spark code to calculate\n",
        "1.\tMin grade of each student\n",
        "2.\tMax grade of each student\n",
        "3.\tGPA \n",
        "4.\tNumber of courses taken\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ybxOeE-4odxr"
      },
      "outputs": [],
      "source": [
        "# Spark installation on Colab\n",
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "# !tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "# !pip install -q findspark\n",
        "# !rm -rf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "\n",
        "!pip install --ignore-installed -q pyspark==3.1.2 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set JAVA_HOME and SPARK_HOME\n",
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"spark-3.0.1-bin-hadoop3.2\"\n",
        "\n",
        "# import findspark\n",
        "# findspark.init(\"spark-3.0.1-bin-hadoop3.2\")# SPARK_HOME\n",
        "\n",
        "\n",
        "import sys\n",
        "import requests\n",
        "from operator import add\n",
        "\n",
        "from pyspark import SparkConf,SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "metadata": {
        "id": "E4ufW6xOor4m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample data\n",
        "data = [(0,0,'James','Smith','CS767',93),\n",
        "  (1,1,'Michael','Rose','MA582',85),\n",
        "  (2,2,'Robert','Williams','CS767',90),\n",
        "  (3,3,'Maria','Jones','CS777',75),\n",
        "  (4,4,'Jen','Brown','CS779',80),\n",
        "  (5,0,'James','Smith','CS555',80),\n",
        "  (6,1,'Michael','Rose','CS555',100)\n",
        "]\n",
        "\n",
        "columns = ['rownumber', 'id', \"firstname\", \"lastname\", \"coursenumber\", \"grade\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)"
      ],
      "metadata": {
        "id": "aO2w_jdQow0q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Mggz8Qpfow5e",
        "outputId": "b5371fba-b1e0-4f1f-fc61-19882b06ee9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+---------+--------+------------+-----+\n",
            "|rownumber| id|firstname|lastname|coursenumber|grade|\n",
            "+---------+---+---------+--------+------------+-----+\n",
            "|        0|  0|    James|   Smith|       CS767|   93|\n",
            "|        1|  1|  Michael|    Rose|       MA582|   85|\n",
            "|        2|  2|   Robert|Williams|       CS767|   90|\n",
            "|        3|  3|    Maria|   Jones|       CS777|   75|\n",
            "|        4|  4|      Jen|   Brown|       CS779|   80|\n",
            "|        5|  0|    James|   Smith|       CS555|   80|\n",
            "|        6|  1|  Michael|    Rose|       CS555|  100|\n",
            "+---------+---+---------+--------+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h__ayALAodxu"
      },
      "outputs": [],
      "source": [
        "minGrade = df.groupBy(\"id\").agg({'grade':'min'})\n",
        "maxGrade = df.groupBy(\"id\").agg({'grade':'max'})\n",
        "GPA = df.groupBy(\"id\").agg({'grade':'mean'})\n",
        "numberOfCourses = df.groupBy(\"id\").agg({'coursenumber':'count'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OsRhqPMRodxu",
        "outputId": "1e53b17f-a673-410a-85c2-8dd4b487a117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| id|min(grade)|\n",
            "+---+----------+\n",
            "|  0|        80|\n",
            "|  1|        85|\n",
            "|  3|        75|\n",
            "|  2|        90|\n",
            "|  4|        80|\n",
            "+---+----------+\n",
            "\n",
            "+---+----------+\n",
            "| id|max(grade)|\n",
            "+---+----------+\n",
            "|  0|        93|\n",
            "|  1|       100|\n",
            "|  3|        75|\n",
            "|  2|        90|\n",
            "|  4|        80|\n",
            "+---+----------+\n",
            "\n",
            "+---+----------+\n",
            "| id|avg(grade)|\n",
            "+---+----------+\n",
            "|  0|      86.5|\n",
            "|  1|      92.5|\n",
            "|  3|      75.0|\n",
            "|  2|      90.0|\n",
            "|  4|      80.0|\n",
            "+---+----------+\n",
            "\n",
            "+---+-------------------+\n",
            "| id|count(coursenumber)|\n",
            "+---+-------------------+\n",
            "|  0|                  2|\n",
            "|  1|                  2|\n",
            "|  3|                  1|\n",
            "|  2|                  1|\n",
            "|  4|                  1|\n",
            "+---+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "minGrade.show()\n",
        "maxGrade.show()\n",
        "GPA.show()\n",
        "numberOfCourses.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYfXHVlRodxu"
      },
      "source": [
        "\n",
        "### Problem 9 (20 points)\n",
        "\n",
        "Estimation area of a circle:\n",
        "\n",
        "Use Spark to estimate area of the unit circle by \"throwing darts\" at the circle. \n",
        "Assume you don’t know how to calculate area of a circle in a closed form, but you know how to calculate area of a square. You throw random darts/points in the 2 by 2 square ((-1, -1) to (1,1)) and count how many falls in the unit circle, a circle with radius of one. The fraction can be used to estimate of the area of the unit circle.\n",
        "\n",
        "Idea of solution:\n",
        "Area of the Unit Circle C= pi*r**2 = pi\n",
        "Area of the square S = 2**2 = 4\n",
        "\n",
        "The fraction of Circle Area by Square Area \n",
        "C/S= Pi/4\n",
        "\n",
        "C = Pi*S/4 = Pi*4/4 = Pi\n",
        "\n",
        "0. Parallelize the data of size n\n",
        "1. throw random darts/points in the 2 by 2 square ((-1, -1) to (1,1))  = get coordinates of points from the Uniform Distribution between -1 and 1\n",
        "2. return true if the point is in a circle using the inequality x*x + y*y < 1, otherwise return False.\n",
        "3. count how many falls in the unit circle = get the proportion of points inside of the circle \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZCS6RWxYodxv",
        "outputId": "f55f7382-5130-456f-fe67-3de95570b0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimation area of the unit circle is 3.13536\n"
          ]
        }
      ],
      "source": [
        "from random import uniform\n",
        "from operator import add\n",
        "\n",
        "n = 100000 \n",
        "\n",
        "def isInCircle(_):\n",
        "    x, y = uniform(-1,1), uniform(-1,1)\n",
        "    return (x * x + y * y < 1)\n",
        "\n",
        "rdd = sc.parallelize(range(1, n + 1))\n",
        "count = rdd.map(isInCircle).reduce(add)\n",
        "print(\"Estimation area of the unit circle is \" + str( 4*count / n))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes\n",
        "This was great introduction to Spark and Hadoop. Initially when I wrote answers to question 1-8, it was easier to feel that I understand what I am doing. In fact, when I got errors compiling the solution for Problem 8, it was tough. Then I got adapted and realized that pyspark is so close to Python. The solution of estimating Pi exists so many years, the question was about how to implement it using map reduce ideas. It was beneficial to solve this assignment in terms of both theory and practice.   "
      ],
      "metadata": {
        "id": "597Xxgtg0B1A"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}