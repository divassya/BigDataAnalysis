{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO6GFVDVW87HYjGY0G22SSS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divassya/BigDataAnalysis/blob/main/AssiyaKaratay_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Info \n",
        "Assignment 4\n",
        "MET CS777 Big Data Analytics\n",
        "\n",
        "Faculty - Farshid Alizadeh-Shabdiz, PhD, MBA\n",
        "\n",
        "Student - Assiya Karatay U95161396 karatay@bu.edu 857-294-7028"
      ],
      "metadata": {
        "id": "Htig3sG60x46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### import libraries"
      ],
      "metadata": {
        "id": "ezcf-q2j1KTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muck9GEM0tKR",
        "outputId": "91c52e37-d3eb-43a4-ed7e-10e606c312f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 67 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 55.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Spark installation on Colab\n",
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "# !tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "# !pip install -q findspark\n",
        "# !rm -rf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "\n",
        "!pip install --ignore-installed -q pyspark==3.1.2 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set JAVA_HOME and SPARK_HOME\n",
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"spark-3.0.1-bin-hadoop3.2\"\n",
        "\n",
        "# import findspark\n",
        "# findspark.init(\"spark-3.0.1-bin-hadoop3.2\")# SPARK_HOME\n",
        "\n",
        "\n",
        "import sys\n",
        "import requests\n",
        "from operator import add\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from pyspark import SparkConf,SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "import re\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "PR7PUYMr0175"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### set up the Google Drive"
      ],
      "metadata": {
        "id": "8uRljevs1Zf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### set up the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f2nVFna1VTa",
        "outputId": "ccfe9ea5-652a-4eea-b5f4-6548730dd865"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choose where project files will be saved\n",
        "project_folder = \"/content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\"\n",
        "# project_folder = sys.argv[2]\n",
        "# change the OS to use the project folder as the working directory\n",
        "os.chdir(project_folder)\n",
        "\n",
        "print('\\n Working directory was changed to ' + project_folder )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGMtloP1X2W",
        "outputId": "17d95dc9-eec9-4d14-d790-4d89ddf2191f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Working directory was changed to /content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1 Generate a 20K dictionary (10 points)"
      ],
      "metadata": {
        "id": "_YBqW38I7c3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 The top 20,000 English words\n",
        "Using Wikipedia pages, find the top 20,000 English words, save them in an array, and sort them based on the frequency of the occurrence."
      ],
      "metadata": {
        "id": "45QO8PUsad19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parsing(lines):\n",
        "  # divide the data defore and after url\n",
        "  division = lines.split('\" url')\n",
        "  # strip all characters except DOC ID\n",
        "  id = division[0].split('<doc id=\"')[1] \n",
        "  # strip the end of title and last 6 characters containing '.</doc' .\n",
        "  text = division[1].split('\">')[1][:-6]\n",
        "  # check the UNICODE decoding for readability \n",
        "  # and lowercase to count words with the same letters together\n",
        "  regex = re.compile(r'[^a-zA-Z]', re.UNICODE).split(text.lower())\n",
        "  return (id,regex)"
      ],
      "metadata": {
        "id": "Hd7UWgpxHGfa"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. get the file\n",
        "wikiPagesFile= project_folder + \"WikipediaPagesOneDocPerLine1000LinesSmall.txt.bz2\"\n",
        "# 2. create an rdd of data\n",
        "wikiPages = sc.textFile(wikiPagesFile)\n",
        "# 3. check if each line has id and url\n",
        "validLines = wikiPages.filter(lambda x : 'id' in x and 'url=' in x)\n",
        "# 4. trim unnecessary symbols and get (docID, listOfWords) pairs\n",
        "listOfWords = validLines.map(parsing)\n",
        "# 5. make every word a tuple dropping DocID\n",
        "wordsAsTuples = listOfWords.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
        "# 6. count number of words in a corpus\n",
        "counts=wordsAsTuples.reduceByKey(lambda x, y: x+y)\n",
        "# 7. get the top 20K in a corpus\n",
        "n = 20000\n",
        "topWords = counts.top(n, lambda x: x[1])\n",
        "# 8. create an empty RDD that has the number 0 through 20000\n",
        "emptyRDD = sc.parallelize(range(n))\n",
        "# 9. order words in descending popularity order, where  \n",
        "# the word in position 0 is the most common used word\n",
        "dictionary = emptyRDD.map (lambda x : (topWords[x][0], x))\n",
        "# 10. save the output \n",
        "dictionary.saveAsTextFile(project_folder+'task11_topEnglishWords')\n",
        "print(\"Word Positions\", dictionary.top(20, lambda x : x[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUIJWH6ZFryD",
        "outputId": "afcf457f-5894-45f3-ef15-eb774cfaed27"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('at', 4761), ('two', 1824), ('priesthood', 78), ('in', 27388), ('', 272655)]\n",
            "Word Positions [('mayhem', 19999), ('hystp', 19998), ('composites', 19997), ('vthl', 19996), ('choisy', 19995), ('ventral', 19994), ('ocelli', 19993), ('humble', 19992), ('lore', 19991), ('fortnights', 19990), ('sunan', 19989), ('mamluk', 19988), ('cartelization', 19987), ('nira', 19986), ('liquidationists', 19985), ('ambivalent', 19984), ('rothbard', 19983), ('tightening', 19982), ('debtor', 19981), ('keynesians', 19980)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 \n",
        "As a result, a dictionary has been generated that contains the top 20K most frequent words in the corpus. Next go over each Wikipedia document and check if the words appear in the Top 20K words. At the end, produce an RDD that includes\n",
        "the docID as key and a Numpy array for the position of each word in the top 20K\n",
        "dictionary.\n",
        "(docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...])"
      ],
      "metadata": {
        "id": "MaiOq8LFaWjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numberOfDocs = wikiPages.count()\n",
        "numberOfDocs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZlrK_i8Fru_",
        "outputId": "754cf67f-13f8-435c-a2e1-f75cd1b05b06"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topWords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avz9e_QPVODx",
        "outputId": "9a5c6892-e280-48dc-f4dc-157634476932"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 73600),\n",
              " ('of', 34092),\n",
              " ('and', 28148),\n",
              " ('in', 27383),\n",
              " ('to', 22234),\n",
              " ('a', 20767),\n",
              " ('was', 11986),\n",
              " ('as', 8704),\n",
              " ('for', 8593),\n",
              " ('is', 8315),\n",
              " ('on', 8277),\n",
              " ('by', 7394),\n",
              " ('s', 7039),\n",
              " ('with', 6999),\n",
              " ('that', 6802),\n",
              " ('he', 5420),\n",
              " ('from', 5149),\n",
              " ('his', 4890),\n",
              " ('at', 4761),\n",
              " ('it', 4387)]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = validLines.first()\n",
        "id = parsing(a)"
      ],
      "metadata": {
        "id": "_1kQJBwOKics"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKbIr7u9QEwV",
        "outputId": "02091b69-6961-42b8-85ed-36173bc19a16"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Postions in our Feature Matrix. Last 20 words in 20k positions:  [('it', 19), ('at', 18), ('his', 17), ('from', 16), ('he', 15), ('that', 14), ('with', 13), ('s', 12), ('by', 11), ('on', 10), ('is', 9), ('for', 8), ('as', 7), ('was', 6), ('a', 5), ('to', 4), ('in', 3), ('and', 2), ('of', 1), ('the', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sc = SparkContext()\n",
        "\n",
        "# Set the file paths on your local machine\n",
        "# Change this line later on your python script when you want to run this on the CLOUD (GC or AWS)\n",
        "\n",
        "wikiPagesFile= \"./WikipediaPagesOneDocPerLine1000LinesSmall.txt.bz2\"\n",
        "# wikiPagesFile = sys.argv[1]\n",
        "wikiCategoryFile=\"./wiki-categorylinks-small.csv.bz2\"\n",
        "# wikiCategoryFile = sys.argv[2]\n",
        "\n",
        "# Read two files into RDDs\n",
        "\n",
        "wikiCategoryLinks=sc.textFile(wikiCategoryFile)\n",
        "\n",
        "wikiCats=wikiCategoryLinks.map(lambda x: x.split(\",\")).map(lambda x: (x[0].replace('\"', ''), x[1].replace('\"', '') ))\n",
        "\n",
        "# Now the wikipages\n",
        "wikiPages = sc.textFile(wikiPagesFile)\n",
        "\n",
        "# Assumption: Each document is stored in one line of the text file\n",
        "# We need this count later ... \n",
        "numberOfDocs = wikiPages.count()\n",
        "\n",
        "print(numberOfDocs)\n",
        "# Each entry in validLines will be a line from the text file\n",
        "validLines = wikiPages.filter(lambda x : 'id' in x and 'url=' in x)\n",
        "\n",
        "# Now, we transform it into a set of (docID, text) pairs\n",
        "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:-6])) \n",
        "\n",
        "#keyAndText.take(1)\n",
        "#validLines.take(1)\n",
        "\n",
        "# Now, we split the text in each (docID, text) pair into a list of words\n",
        "# After this step, we have a data set with\n",
        "# (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
        "# We use a regular expression here to make\n",
        "# sure that the program does not break down on some of the documents\n",
        "\n",
        "regex = re.compile('[^a-zA-Z]')\n",
        "\n",
        "# remove all non letter characters\n",
        "keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
        "# better solution here is to use NLTK tokenizer\n",
        "\n",
        "# Now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
        "# to (\"word1\", 1) (\"word2\", 1)...\n",
        "allWords = keyAndListOfWords.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
        "allWords.take(10)\n",
        "\n",
        "# Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
        "allCounts = allWords.reduceByKey(add)\n",
        "#allCounts.take(10)\n",
        "\n",
        "# Get the top 20,000 words in a local array in a sorted format based on frequency\n",
        "# If you want to run it on your laptio, it may a longer time for top 20k words. \n",
        "topWords = allCounts.top(20000, lambda x: x[1])\n",
        "\n",
        "print(\"Top Words in Corpus:\", allCounts.top(10, key=lambda x: x[1]))\n",
        "\n",
        "# We'll create a RDD that has a set of (word, dictNum) pairs\n",
        "# start by creating an RDD that has the number 0 through 20000\n",
        "# 20000 is the number of words that will be in our dictionary\n",
        "topWordsK = sc.parallelize(range(20000))\n",
        "\n",
        "# Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 1)\n",
        "# (\"NextMostCommon\", 2), ...\n",
        "# the number will be the spot in the dictionary used to tell us\n",
        "# where the word is located\n",
        "dictionary = topWordsK.map (lambda x : (topWords[x][0], x))\n",
        "\n",
        "\n",
        "print(\"Word Postions in our Feature Matrix. Last 20 words in 20k positions: \", dictionary.top(20, lambda x : x[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "3UuokvrI6F-C",
        "outputId": "b139e6f8-e358-4d75-aedd-e382f8bccf61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ab235aaee703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Assumption: Each document is stored in one line of the text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# We need this count later ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mnumberOfDocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikiPages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumberOfDocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \"\"\"\n\u001b[0;32m-> 1235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \"\"\"\n\u001b[0;32m-> 1224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/WikipediaPagesOneDocPerLine1000LinesSmall.txt.bz2\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkWbI2O27i5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GYZskPv7i8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VT_pbBmU7i_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 - Create the TF-IDF Array (20 Points)\n",
        "Boston University Metropolitan College\n",
        "After having the top 20K words we want to create a large array that its\n",
        "columns are the words of the dictionary with number of occurrences of each word\n",
        "and the rows are documents.\n",
        "The first step is calculating the “Term Frequency”, TF (x, w), vector for each\n",
        "document as follows:\n",
        "“Term Frequency” is an indication of the number of times a term occurs in a document.\n",
        "Numerator is number of occurrences of a word, and the denominator is the sum of\n",
        "all the words of the document.\n",
        "Next, calculate “Inverse Document Frequency” for all the documents and finally\n",
        "calculate TF-IDF(w) and create TF-IDF matrix of the corpus:\n",
        "Note that the “size of corpus” is total number of documents (numerator).\n",
        "To learn more about TF-IDF see the Wikipedia page:\n",
        "https://en.wikipedia.org/wiki/Tf-idf\n",
        "Task 3 - Implement the getPrediction function (30 Points)\n",
        "Finally, implement the function getPrediction(textInput, k), which will predict the\n",
        "membership of the textInput to the top 20 closest documents, and the list of top\n",
        "categories.\n",
        "You should use the cosine similarity to calculate the distances.\n",
        "Boston University Metropolitan College\n",
        "Task 4 – Implement the code using Dataframes (30 points)\n",
        "Implement the complete code in Dataframe and print out the results of the task 3\n",
        "using dataframes in pyspark. From the beginning of your code to the end of your\n",
        "kNN implementation you are allowed to use spark dataframe and python (including\n",
        "python libraries like numpy). You are not allowed to use RDDs.\n",
        "Task 5 - Removing Stop Words and Do Stemming (10 points)\n",
        "Task 5.1 - Remove Stop Words (5 point)\n",
        "Describe if removing the English Stop words (most common words like ”a,\n",
        "the, is, are, i, you, ...”) would change the final kNN results.\n",
        "Does your result change significantly after removing the stop words? Why?\n",
        "Provide reasons.\n",
        "You do not need to implement this task.\n",
        "Task 5.2 – Considering English word stemming (5 point)\n",
        "We can stem the words [”game”,”gaming”,”gamed”,”games”] to their root\n",
        "word ”game”.\n",
        "Does stemming change your result significantly? Why? Provide reasons.\n",
        "You can learn more about stemming at:\n",
        "https://en.wikipedia.org/wiki/Stemming\n",
        "You do not need to implement this task."
      ],
      "metadata": {
        "id": "8bbB-O7K7jQs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFmIaC6e7tjq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}