{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8230WKq8ooY29dAVHi4AO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divassya/BigDataAnalysis/blob/main/AssiyaKaratay_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Info \n",
        "Assignment 4\n",
        "MET CS777 Big Data Analytics\n",
        "\n",
        "Faculty - Farshid Alizadeh-Shabdiz, PhD, MBA\n",
        "\n",
        "Student - Assiya Karatay U95161396 karatay@bu.edu 857-294-7028"
      ],
      "metadata": {
        "id": "Htig3sG60x46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### import libraries"
      ],
      "metadata": {
        "id": "ezcf-q2j1KTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muck9GEM0tKR",
        "outputId": "c971b77f-a325-41ac-8606-919f6c2eb2ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 78 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 80.3 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Spark installation on Colab\n",
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "# !tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "# !pip install -q findspark\n",
        "# !rm -rf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "\n",
        "!pip install --ignore-installed -q pyspark==3.1.2 "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set JAVA_HOME and SPARK_HOME\n",
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"spark-3.0.1-bin-hadoop3.2\"\n",
        "\n",
        "# import findspark\n",
        "# findspark.init(\"spark-3.0.1-bin-hadoop3.2\")# SPARK_HOME\n",
        "\n",
        "\n",
        "import sys\n",
        "import requests\n",
        "import numpy as np\n",
        "from operator import add\n",
        "\n",
        "from pyspark import SparkConf,SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "import re\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "PR7PUYMr0175"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### set up the Google Drive"
      ],
      "metadata": {
        "id": "8uRljevs1Zf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### set up the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f2nVFna1VTa",
        "outputId": "c43a7067-2b29-4705-d937-57ae314b15ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choose where project files will be saved\n",
        "project_folder = \"/content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\"\n",
        "# project_folder = sys.argv[2]\n",
        "# change the OS to use the project folder as the working directory\n",
        "os.chdir(project_folder)\n",
        "\n",
        "print('\\n Working directory was changed to ' + project_folder )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGMtloP1X2W",
        "outputId": "52ee124c-644a-46ad-d28b-a37e2d81daa0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Working directory was changed to /content/drive/MyDrive/CS777_BigDataAnalytics/Assignment4/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1 Generate a 20K dictionary (10 points)"
      ],
      "metadata": {
        "id": "_YBqW38I7c3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 The top 20,000 English words\n",
        "Using Wikipedia pages, find the top 20,000 English words, save them in an array, and sort them based on the frequency of the occurrence."
      ],
      "metadata": {
        "id": "45QO8PUsad19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parsing(lines):\n",
        "  # divide the data defore and after url\n",
        "  division = lines.split('\" url')\n",
        "  # strip all characters except DOC ID\n",
        "  id = division[0].split('<doc id=\"')[1] \n",
        "  # strip the end of title and last 6 characters containing '.</doc' .\n",
        "  text = division[1].split('\">')[1][:-6]\n",
        "  # check the UNICODE decoding for readability \n",
        "  # and lowercase to count words with the same letters together\n",
        "  regex = re.compile(r'[^a-zA-Z]', re.UNICODE).split(text.lower())\n",
        "  return (id,regex)"
      ],
      "metadata": {
        "id": "Hd7UWgpxHGfa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. get the file\n",
        "wikiPagesFile= project_folder + \"WikipediaPagesOneDocPerLine1000LinesSmall.txt.bz2\"\n",
        "# 2. create an rdd of data\n",
        "wikiPages = sc.textFile(wikiPagesFile)\n",
        "# 3. check if each line has id and url\n",
        "validLines = wikiPages.filter(lambda x : 'id' in x and 'url=' in x)\n",
        "# 4. trim unnecessary symbols and get (docID, listOfWords) pairs\n",
        "listOfWords = validLines.map(parsing)\n",
        "# 5. make every word a tuple dropping DocID\n",
        "wordsAsTuples = listOfWords.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
        "# 6. count number of words in a corpus\n",
        "counts=wordsAsTuples.reduceByKey(lambda x, y: x+y)\n",
        "# 7. get the top 20K in a corpus\n",
        "n = 20000\n",
        "topWords = counts.top(n, lambda x: x[1])\n",
        "# 8. create an empty RDD that has the number 0 through 20000\n",
        "emptyRDD = sc.parallelize(range(n))\n",
        "# 9. order words in descending popularity order, where  \n",
        "# the word in position 0 is the most common used word\n",
        "dictionary = emptyRDD.map (lambda x : (topWords[x][0], x))\n",
        "# 10. save the output \n",
        "dictionary.saveAsTextFile(project_folder+'task11_topEnglishWords')\n",
        "print(\"The top 20,000 English words\", dictionary.top(5, lambda x : -x[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUIJWH6ZFryD",
        "outputId": "c592b0dc-7269-4620-c743-517f67ec5153"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top 20,000 English words [('', 0), ('the', 1), ('of', 2), ('and', 3), ('in', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 docID as key and a Numpy array for the position of each word\n",
        "As a result, a dictionary has been generated that contains the top 20K most frequent words in the corpus. Next go over each Wikipedia document and check if the words appear in the Top 20K words. At the end, produce an RDD that includes\n",
        "the docID as key and a Numpy array for the position of each word in the top 20K dictionary.\n",
        "(docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...])"
      ],
      "metadata": {
        "id": "MaiOq8LFaWjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numberOfOccurrences(docIDAndPos):\n",
        "    docID = docIDAndPos[0]\n",
        "    listOfIndices = docIDAndPos[1]\n",
        "    # create an array of zeros\n",
        "    returnVal = np.zeros(20000)\n",
        "    # count the occurrence of words e.g. there are 514 'my' in docID1, where \n",
        "    # 'my' is in the position 0 in the corpus\n",
        "    for index in listOfIndices:\n",
        "        returnVal[index] = returnVal[index] + 1\n",
        "    return (docID, returnVal)"
      ],
      "metadata": {
        "id": "Mlz9BDSUVxzU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. get (\"word1\", docID) pairs from (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
        "allWordsWithDocID = listOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
        "# 2. inner join with dictionary to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
        "allDictionaryWords = dictionary.join(allWordsWithDocID)\n",
        "# 3. drop the word as string to get a set of (docID, dictionaryPos) pairs\n",
        "docIDAndPos = allDictionaryWords.map(lambda x: (x[1][1], x[1][0]))\n",
        "# 4. get a pair (docID, np.array('count of word in pos1', 'count of word in pos2',...))\n",
        "allDictionaryWordsInDocID = docIDAndPos.groupByKey()\n",
        "IDPosArray = allDictionaryWordsInDocID.map(numberOfOccurrences)\n",
        "# 5. save the output\n",
        "IDPosArray.saveAsTextFile(project_folder+'task12_wordOccurrences')"
      ],
      "metadata": {
        "id": "gx4YALyzRVoa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2 - Create the TF-IDF Array (20 Points)\n",
        "#### TF\n",
        "After having the top 20K words we want to create a large array that its\n",
        "columns are the words of the dictionary with number of occurrences of each word and the rows are documents.\n",
        "The first step is calculating the “Term Frequency”, TF (x, w), vector for each document as follows:\n",
        "“Term Frequency” is an indication of the number of times a term occurs in a document.\n",
        "Numerator is number of occurrences of a word, and the denominator is the sum of all the words of the document."
      ],
      "metadata": {
        "id": "mbSuv2tWVyWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def termFrequency(IDPosArray):\n",
        "    numberOfWords = np.sum(IDPosArray[1])\n",
        "    returnVal = np.divide(IDPosArray[1], numberOfWords)\n",
        "    return (IDPosArray[0], returnVal)\n",
        "# Now get (docID, [TF1, TF2, ...]) pairs\n",
        "tf = IDPosArray.map(termFrequency)"
      ],
      "metadata": {
        "id": "EoCukGAJnV1c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IDF\n",
        "Next, calculate “Inverse Document Frequency” for all the documents and finally\n",
        "calculate TF-IDF(w) and create TF-IDF matrix of the corpus:\n",
        "Note that the “size of corpus” is total number of documents (numerator).\n",
        "To learn more about TF-IDF see the Wikipedia page:\n",
        "https://en.wikipedia.org/wiki/Tf-idf"
      ],
      "metadata": {
        "id": "-6x5eGRLeVAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def occurrenceSign(tf):\n",
        "    # empty np array of size 20K\n",
        "    returnVal = np.zeros (20000)\n",
        "    # get positions in dictionary of which words occurred in a doc\n",
        "    tfZero = np.where(tf[1]>0)[0]\n",
        "    for index in tfZero:\n",
        "        if returnVal[index] == 0: \n",
        "          returnVal[index] = 1\n",
        "    # A zero means that the word does not occur,one means that it does.\n",
        "    return (tf[0], returnVal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erd_JAqcwklK",
        "outputId": "3219fe5e-8835-4e3a-ad9f-c13984f66cab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('431952', array([1., 1., 1., ..., 0., 0., 0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. build occurrence table of words as 1 and 0\n",
        "zeroOrOne = tf.map(occurrenceSign)\n",
        "# 2. count the number of documents where each word in a dict appeared\n",
        "dfArray = zeroOrOne.values().sum()\n",
        "# 3. create an array of numberOfDocs of size 20K\n",
        "numberOfDocs = wikiPages.count()\n",
        "multiplier = np.full(n, numberOfDocs)\n",
        "# 4. get the version of dfArray where the i^th entry is the idf for the i^th word in the corpus\n",
        "idfArray = np.log(np.divide(multiplier, dfArray))\n",
        "# 5. convert all of the tf vectors to tf*idf vectors\n",
        "tfidf = tf.map(lambda x: (x[0], np.multiply(x[1], idfArray)))\n",
        "# 6. save the output\n",
        "tfidf.saveAsTextFile(project_folder+'task2_tfidf')"
      ],
      "metadata": {
        "id": "kYCKvwvKlGav"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.first()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKUgUVS3zPBh",
        "outputId": "daaf04df-fb11-48be-b9f6-22705e5df9ea"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('431952',\n",
              " array([0.        , 0.00576616, 0.00189731, ..., 0.        , 0.        ,\n",
              "        0.        ]))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3 - Implement the getPrediction function (30 Points)\n",
        "Finally, implement the function getPrediction(textInput, k), which will predict the\n",
        "membership of the textInput to the top 20 closest documents, and the list of top\n",
        "categories.\n",
        "You should use the cosine similarity to calculate the distances."
      ],
      "metadata": {
        "id": "hwojfp4ewFOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parsingCategories(lines):\n",
        "  # separate the data before and after comma\n",
        "  division = lines.replace('\"', '').split(',')\n",
        "  return (division[0],division[1])"
      ],
      "metadata": {
        "id": "7gl2hZ_Ownbm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikiCategoryFile=project_folder + \"wiki-categorylinks-small.csv.bz2\"\n",
        "# wikiCategoryFile = sys.argv[2]\n",
        "wikiCategoryLinks=sc.textFile(wikiCategoryFile)\n",
        "\n",
        "wikiCats = wikiCategoryLinks.map(parsingCategories)\n",
        "# join tfidf with categories\n",
        "catsTFIDFJoined = wikiCats.join(tfidf)\n",
        "# obtain category name and tfidf array\n",
        "featuresRDD = catsTFIDFJoined.map(lambda x: (x[1][0], x[1][1]))\n",
        "# Cache this data because we need to run kNN on this data set\n",
        "featuresRDD.cache()\n",
        "print(featuresRDD.first())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkZoxk4QjwRo",
        "outputId": "7962bfe1-ba68-409c-85af-34cec58bbec2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('All_articles_with_unsourced_statements', array([0.        , 0.00404342, 0.00183544, ..., 0.        , 0.        ,\n",
            "       0.        ]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textInput = 'How many goals Vancouver score last year?'\n",
        "myDoc = sc.parallelize (('', textInput))\n",
        "# Flat map the text to (word, 1) pair for each word in the doc\n",
        "wordsInThatDoc = myDoc.flatMap (lambda x : ((j, 1) for j in re.compile(r'[^a-zA-Z]', re.UNICODE).sub(' ', x).lower().split()))\n",
        "# This will give us a set of (word, (dictionaryPos, 1)) pairs\n",
        "myDocAndDictJoined = dictionary.join (wordsInThatDoc)\n",
        "# drop the string word and have (1,dictionaryPos)\n",
        "allDictionaryWordsInThatDoc = myDocAndDictJoined.map (lambda x: (x[1][1], x[1][0])).groupByKey()  \n",
        "IDPosArray = allDictionaryWordsInThatDoc.map(numberOfOccurrences)\n",
        "# Get tf array for the input string\n",
        "tf = IDPosArray.map(termFrequency).collect()[0][1]"
      ],
      "metadata": {
        "id": "CXWfTPXqwnXP"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myArray = np.multiply (tf, idfArray)\n",
        "print(len(myArray))\n",
        "\n",
        "# Get the distance from the input text string to all database documents, using cosine similarity (np.dot() )\n",
        "distances = featuresRDD.map (lambda x : (x[0], np.dot (x[1], myArray)))\n",
        "\n",
        "# get the top k distances\n",
        "k = 10\n",
        "topK = distances.top (k, lambda x : x[1])\n",
        "print(topK)\n",
        "\n",
        "# and transform the top k distances into a set of (docID, 1) pairs\n",
        "docIDRepresented = sc.parallelize(topK).map (lambda x : (x[0], 1))\n",
        "\n",
        "# now, for each docID, get the count of the number of times this document ID appeared in the top k\n",
        "numTimes = docIDRepresented.reduceByKey(add)\n",
        "    \n",
        "# Return the top 1 of them.\n",
        "# Ask yourself: Why we are using twice top() operation here?\n",
        "# Answer: to show them in sorted order based on number of times\n",
        "# return numTimes.top(k, lambda x: x[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUeRB9TIkGzu",
        "outputId": "d67063de-38c4-4080-cf05-b54966eb6622"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n",
            "[('1990s_Canadian_teen_drama_television_series', 0.05929690167748058), ('1991_Canadian_television_series_debuts', 0.05929690167748058), ('1994_Canadian_television_series_endings', 0.05929690167748058), ('All_stub_articles', 0.05929690167748058), ('CBC_Television_shows', 0.05929690167748058), ('Canadian_television_program_stubs', 0.05929690167748058), ('Television_shows_set_in_Vancouver', 0.05929690167748058), ('1979_births', 0.041490962048343696), ('Ak_Bars_Kazan_players', 0.041490962048343696), ('All_stub_articles', 0.041490962048343696)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numTimes.top(k, lambda x: x[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO-2KO0nkG2O",
        "outputId": "06851adb-1066-493d-aa1a-599e4d43701e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('All_stub_articles', 2),\n",
              " ('1994_Canadian_television_series_endings', 1),\n",
              " ('CBC_Television_shows', 1),\n",
              " ('1979_births', 1),\n",
              " ('1990s_Canadian_teen_drama_television_series', 1),\n",
              " ('1991_Canadian_television_series_debuts', 1),\n",
              " ('Canadian_television_program_stubs', 1),\n",
              " ('Television_shows_set_in_Vancouver', 1),\n",
              " ('Ak_Bars_Kazan_players', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a function that returns the prediction for the label of a string, using a kNN algorithm\n",
        "def getPrediction (textInput, k):\n",
        "    # Create an RDD out of the textIput\n",
        "    myDoc = sc.parallelize (('', textInput))\n",
        "\n",
        "    # Flat map the text to (word, 1) pair for each word in the doc\n",
        "    wordsInThatDoc = myDoc.flatMap (lambda x : ((j, 1) for j in regex.sub(' ', x).lower().split()))\n",
        "    \n",
        "    # This will give us a set of (word, (dictionaryPos, 1)) pairs\n",
        "    allDictionaryWordsInThatDoc = dictionary.join (wordsInThatDoc).map (lambda x: (x[1][1], x[1][0])).groupByKey ()\n",
        "    \n",
        "    # Get tf array for the input string\n",
        "    myArray = buildArray (allDictionaryWordsInThatDoc.top (1)[0][1])\n",
        "    \n",
        "    # Get the tf * idf array for the input string\n",
        "    myArray = np.multiply (myArray, idfArray)\n",
        "\n",
        "    # Get the distance from the input text string to all database documents, using cosine similarity (np.dot() )\n",
        "    distances = featuresRDD.map (lambda x : (x[0], np.dot (x[1], myArray)))\n",
        "    \n",
        "    # distances = allDocsAsNumpyArraysTFidf.map (lambda x : (x[0], cousinSim (x[1],myArray)))\n",
        "    # get the top k distances\n",
        "    topK = distances.top (k, lambda x : x[1])\n",
        "    \n",
        "    # and transform the top k distances into a set of (docID, 1) pairs\n",
        "    docIDRepresented = sc.parallelize(topK).map (lambda x : (x[0], 1))\n",
        "    \n",
        "    # now, for each docID, get the count of the number of times this document ID appeared in the top k\n",
        "    numTimes = docIDRepresented.reduceByKey(add)\n",
        "        \n",
        "    # Return the top 1 of them.\n",
        "    # Ask yourself: Why we are using twice top() operation here?\n",
        "    # Answer: to show them in sorted order based on number of times\n",
        "    return numTimes.top(k, lambda x: x[1])\n",
        "\n",
        "print(getPrediction('Sport Basketball Volleyball Soccer', 10))\n",
        "\n",
        "print(getPrediction('What is the capital city of Australia?', 10))\n",
        "\n",
        "print(getPrediction('How many goals Vancouver score last year?', 10))"
      ],
      "metadata": {
        "id": "7Rj4j3uOkG5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkWbI2O27i5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GYZskPv7i8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VT_pbBmU7i_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Task 4 – Implement the code using Dataframes (30 points)\n",
        "Implement the complete code in Dataframe and print out the results of the task 3\n",
        "using dataframes in pyspark. From the beginning of your code to the end of your\n",
        "kNN implementation you are allowed to use spark dataframe and python (including\n",
        "python libraries like numpy). You are not allowed to use RDDs.\n",
        "Task 5 - Removing Stop Words and Do Stemming (10 points)\n",
        "Task 5.1 - Remove Stop Words (5 point)\n",
        "Describe if removing the English Stop words (most common words like ”a,\n",
        "the, is, are, i, you, ...”) would change the final kNN results.\n",
        "Does your result change significantly after removing the stop words? Why?\n",
        "Provide reasons.\n",
        "You do not need to implement this task.\n",
        "Task 5.2 – Considering English word stemming (5 point)\n",
        "We can stem the words [”game”,”gaming”,”gamed”,”games”] to their root\n",
        "word ”game”.\n",
        "Does stemming change your result significantly? Why? Provide reasons.\n",
        "You can learn more about stemming at:\n",
        "https://en.wikipedia.org/wiki/Stemming\n",
        "You do not need to implement this task."
      ],
      "metadata": {
        "id": "8bbB-O7K7jQs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFmIaC6e7tjq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}